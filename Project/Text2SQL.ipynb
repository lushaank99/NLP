{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "xLUwviThTzXw"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "pnvsb3iIRUqp"
   },
   "outputs": [],
   "source": [
    "import os,json,collections,copy,math\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "5f1V5Pb4WdGF"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nhF1ybAtXOAo",
    "outputId": "5b71a103-fe9e-4781-c591-3e1a33d23ac8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LaXAjR5nazGR"
   },
   "source": [
    "# Dataset loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "tLdNb8NVRpy6"
   },
   "outputs": [],
   "source": [
    "def get_data(path_wiki,mode='train'):\n",
    "    sql_path = os.path.join(path_wiki,mode+'_knowledge.jsonl')\n",
    "    table_path = os.path.join(path_wiki+\"data/\",mode+'.tables.jsonl')\n",
    "\n",
    "    data=[]\n",
    "    table={}\n",
    "\n",
    "    #Getting Questions and SQL Queries\n",
    "    with open(sql_path) as f :\n",
    "        for idx,line in enumerate(f):\n",
    "            t1 = json.loads(line.strip())\n",
    "            data.append(t1)\n",
    "\n",
    "    #Getting SQL Tables        \n",
    "    with open(table_path) as f:\n",
    "        for idx, line in enumerate(f):\n",
    "            t1 = json.loads(line.strip())\n",
    "            table[t1['id']] = t1\n",
    "    return data,table\n",
    " \n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "GUkz-zvhTHpM",
    "outputId": "92fc0721-ec22-4ae7-95f3-739a971894b7"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'/content/drive/My Drive/Text2SQL/'"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fileDir = os.path.dirname(os.path.realpath('__file__'))\n",
    "path_wiki = '/content/drive/My Drive/Text2SQL/'\n",
    "path_wiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "cs1dJ_ptR0Ku"
   },
   "outputs": [],
   "source": [
    "train_data,train_table = get_data(path_wiki)\n",
    "dev_data,dev_table = get_data(path_wiki,mode='dev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Y-h16-iYh8N6"
   },
   "outputs": [],
   "source": [
    "def create_dataloader(train_data, dev_data,batch_size, shuffle_train=True, shuffle_dev=False):\n",
    "  train_loader = torch.utils.data.DataLoader(\n",
    "        batch_size=batch_size,\n",
    "        dataset=train_data,\n",
    "        shuffle=shuffle_train,\n",
    "        num_workers=4,\n",
    "        collate_fn=lambda x: x  # now dictionary values are not merged!\n",
    "    )\n",
    "  dev_loader = torch.utils.data.DataLoader(\n",
    "        batch_size=batch_size,\n",
    "        dataset=dev_data,\n",
    "        shuffle=shuffle_dev,\n",
    "        num_workers=4,\n",
    "        collate_fn=lambda x: x  # now dictionary values are not merged!\n",
    "    )\n",
    "\n",
    "  return train_loader, dev_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "BxiCWsSjl7ZL"
   },
   "outputs": [],
   "source": [
    "# batch_size=8\n",
    "batch_size=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "fL471QSwlcSq"
   },
   "outputs": [],
   "source": [
    "\n",
    "train_loader,dev_loader = create_dataloader(train_data,dev_data,batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "odwLB5TSq5jF",
    "outputId": "d03f6a28-7a90-430e-de20-de671b935bdf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5636"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sam3o4sia99G"
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Obw5SLXsmA9h"
   },
   "outputs": [],
   "source": [
    "def load_vocab(vocab_file):\n",
    "    \"\"\"Loads a vocabulary file into a dictionary.\"\"\"\n",
    "    vocab = collections.OrderedDict()\n",
    "    index = 0\n",
    "    with open(vocab_file, \"r\", encoding=\"utf-8\") as reader:\n",
    "        while True:\n",
    "            # token = convert_to_unicode(reader.readline())\n",
    "            token = reader.readline()\n",
    "            if not token:\n",
    "                break\n",
    "            token = token.strip()\n",
    "            vocab[token] = index\n",
    "            index += 1\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "iA_8kbP-2ouB"
   },
   "outputs": [],
   "source": [
    "vocab_file='/content/drive/My Drive/Text2SQL/vocab_uncased_L-12_H-768_A-12.txt'\n",
    "vocab = load_vocab(vocab_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XmneNKRl25WG",
    "outputId": "4e345b8d-846d-41e4-ca33-30a82f82b29f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('[PAD]', 0),\n",
       "             ('[unused0]', 1),\n",
       "             ('[unused1]', 2),\n",
       "             ('[unused2]', 3),\n",
       "             ('[unused3]', 4),\n",
       "             ('[unused4]', 5),\n",
       "             ('[unused5]', 6),\n",
       "             ('[unused6]', 7),\n",
       "             ('[unused7]', 8),\n",
       "             ('[unused8]', 9),\n",
       "             ('[unused9]', 10),\n",
       "             ('[unused10]', 11),\n",
       "             ('[unused11]', 12),\n",
       "             ('[unused12]', 13),\n",
       "             ('[unused13]', 14),\n",
       "             ('[unused14]', 15),\n",
       "             ('[unused15]', 16),\n",
       "             ('[unused16]', 17),\n",
       "             ('[unused17]', 18),\n",
       "             ('[unused18]', 19),\n",
       "             ('[unused19]', 20),\n",
       "             ('[unused20]', 21),\n",
       "             ('[unused21]', 22),\n",
       "             ('[unused22]', 23),\n",
       "             ('[unused23]', 24),\n",
       "             ('[unused24]', 25),\n",
       "             ('[unused25]', 26),\n",
       "             ('[unused26]', 27),\n",
       "             ('[unused27]', 28),\n",
       "             ('[unused28]', 29),\n",
       "             ('[unused29]', 30),\n",
       "             ('[unused30]', 31),\n",
       "             ('[unused31]', 32),\n",
       "             ('[unused32]', 33),\n",
       "             ('[unused33]', 34),\n",
       "             ('[unused34]', 35),\n",
       "             ('[unused35]', 36),\n",
       "             ('[unused36]', 37),\n",
       "             ('[unused37]', 38),\n",
       "             ('[unused38]', 39),\n",
       "             ('[unused39]', 40),\n",
       "             ('[unused40]', 41),\n",
       "             ('[unused41]', 42),\n",
       "             ('[unused42]', 43),\n",
       "             ('[unused43]', 44),\n",
       "             ('[unused44]', 45),\n",
       "             ('[unused45]', 46),\n",
       "             ('[unused46]', 47),\n",
       "             ('[unused47]', 48),\n",
       "             ('[unused48]', 49),\n",
       "             ('[unused49]', 50),\n",
       "             ('[unused50]', 51),\n",
       "             ('[unused51]', 52),\n",
       "             ('[unused52]', 53),\n",
       "             ('[unused53]', 54),\n",
       "             ('[unused54]', 55),\n",
       "             ('[unused55]', 56),\n",
       "             ('[unused56]', 57),\n",
       "             ('[unused57]', 58),\n",
       "             ('[unused58]', 59),\n",
       "             ('[unused59]', 60),\n",
       "             ('[unused60]', 61),\n",
       "             ('[unused61]', 62),\n",
       "             ('[unused62]', 63),\n",
       "             ('[unused63]', 64),\n",
       "             ('[unused64]', 65),\n",
       "             ('[unused65]', 66),\n",
       "             ('[unused66]', 67),\n",
       "             ('[unused67]', 68),\n",
       "             ('[unused68]', 69),\n",
       "             ('[unused69]', 70),\n",
       "             ('[unused70]', 71),\n",
       "             ('[unused71]', 72),\n",
       "             ('[unused72]', 73),\n",
       "             ('[unused73]', 74),\n",
       "             ('[unused74]', 75),\n",
       "             ('[unused75]', 76),\n",
       "             ('[unused76]', 77),\n",
       "             ('[unused77]', 78),\n",
       "             ('[unused78]', 79),\n",
       "             ('[unused79]', 80),\n",
       "             ('[unused80]', 81),\n",
       "             ('[unused81]', 82),\n",
       "             ('[unused82]', 83),\n",
       "             ('[unused83]', 84),\n",
       "             ('[unused84]', 85),\n",
       "             ('[unused85]', 86),\n",
       "             ('[unused86]', 87),\n",
       "             ('[unused87]', 88),\n",
       "             ('[unused88]', 89),\n",
       "             ('[unused89]', 90),\n",
       "             ('[unused90]', 91),\n",
       "             ('[unused91]', 92),\n",
       "             ('[unused92]', 93),\n",
       "             ('[unused93]', 94),\n",
       "             ('[unused94]', 95),\n",
       "             ('[unused95]', 96),\n",
       "             ('[unused96]', 97),\n",
       "             ('[unused97]', 98),\n",
       "             ('[unused98]', 99),\n",
       "             ('[UNK]', 100),\n",
       "             ('[CLS]', 101),\n",
       "             ('[SEP]', 102),\n",
       "             ('[MASK]', 103),\n",
       "             ('[unused99]', 104),\n",
       "             ('[unused100]', 105),\n",
       "             ('[unused101]', 106),\n",
       "             ('[unused102]', 107),\n",
       "             ('[unused103]', 108),\n",
       "             ('[unused104]', 109),\n",
       "             ('[unused105]', 110),\n",
       "             ('[unused106]', 111),\n",
       "             ('[unused107]', 112),\n",
       "             ('[unused108]', 113),\n",
       "             ('[unused109]', 114),\n",
       "             ('[unused110]', 115),\n",
       "             ('[unused111]', 116),\n",
       "             ('[unused112]', 117),\n",
       "             ('[unused113]', 118),\n",
       "             ('[unused114]', 119),\n",
       "             ('[unused115]', 120),\n",
       "             ('[unused116]', 121),\n",
       "             ('[unused117]', 122),\n",
       "             ('[unused118]', 123),\n",
       "             ('[unused119]', 124),\n",
       "             ('[unused120]', 125),\n",
       "             ('[unused121]', 126),\n",
       "             ('[unused122]', 127),\n",
       "             ('[unused123]', 128),\n",
       "             ('[unused124]', 129),\n",
       "             ('[unused125]', 130),\n",
       "             ('[unused126]', 131),\n",
       "             ('[unused127]', 132),\n",
       "             ('[unused128]', 133),\n",
       "             ('[unused129]', 134),\n",
       "             ('[unused130]', 135),\n",
       "             ('[unused131]', 136),\n",
       "             ('[unused132]', 137),\n",
       "             ('[unused133]', 138),\n",
       "             ('[unused134]', 139),\n",
       "             ('[unused135]', 140),\n",
       "             ('[unused136]', 141),\n",
       "             ('[unused137]', 142),\n",
       "             ('[unused138]', 143),\n",
       "             ('[unused139]', 144),\n",
       "             ('[unused140]', 145),\n",
       "             ('[unused141]', 146),\n",
       "             ('[unused142]', 147),\n",
       "             ('[unused143]', 148),\n",
       "             ('[unused144]', 149),\n",
       "             ('[unused145]', 150),\n",
       "             ('[unused146]', 151),\n",
       "             ('[unused147]', 152),\n",
       "             ('[unused148]', 153),\n",
       "             ('[unused149]', 154),\n",
       "             ('[unused150]', 155),\n",
       "             ('[unused151]', 156),\n",
       "             ('[unused152]', 157),\n",
       "             ('[unused153]', 158),\n",
       "             ('[unused154]', 159),\n",
       "             ('[unused155]', 160),\n",
       "             ('[unused156]', 161),\n",
       "             ('[unused157]', 162),\n",
       "             ('[unused158]', 163),\n",
       "             ('[unused159]', 164),\n",
       "             ('[unused160]', 165),\n",
       "             ('[unused161]', 166),\n",
       "             ('[unused162]', 167),\n",
       "             ('[unused163]', 168),\n",
       "             ('[unused164]', 169),\n",
       "             ('[unused165]', 170),\n",
       "             ('[unused166]', 171),\n",
       "             ('[unused167]', 172),\n",
       "             ('[unused168]', 173),\n",
       "             ('[unused169]', 174),\n",
       "             ('[unused170]', 175),\n",
       "             ('[unused171]', 176),\n",
       "             ('[unused172]', 177),\n",
       "             ('[unused173]', 178),\n",
       "             ('[unused174]', 179),\n",
       "             ('[unused175]', 180),\n",
       "             ('[unused176]', 181),\n",
       "             ('[unused177]', 182),\n",
       "             ('[unused178]', 183),\n",
       "             ('[unused179]', 184),\n",
       "             ('[unused180]', 185),\n",
       "             ('[unused181]', 186),\n",
       "             ('[unused182]', 187),\n",
       "             ('[unused183]', 188),\n",
       "             ('[unused184]', 189),\n",
       "             ('[unused185]', 190),\n",
       "             ('[unused186]', 191),\n",
       "             ('[unused187]', 192),\n",
       "             ('[unused188]', 193),\n",
       "             ('[unused189]', 194),\n",
       "             ('[unused190]', 195),\n",
       "             ('[unused191]', 196),\n",
       "             ('[unused192]', 197),\n",
       "             ('[unused193]', 198),\n",
       "             ('[unused194]', 199),\n",
       "             ('[unused195]', 200),\n",
       "             ('[unused196]', 201),\n",
       "             ('[unused197]', 202),\n",
       "             ('[unused198]', 203),\n",
       "             ('[unused199]', 204),\n",
       "             ('[unused200]', 205),\n",
       "             ('[unused201]', 206),\n",
       "             ('[unused202]', 207),\n",
       "             ('[unused203]', 208),\n",
       "             ('[unused204]', 209),\n",
       "             ('[unused205]', 210),\n",
       "             ('[unused206]', 211),\n",
       "             ('[unused207]', 212),\n",
       "             ('[unused208]', 213),\n",
       "             ('[unused209]', 214),\n",
       "             ('[unused210]', 215),\n",
       "             ('[unused211]', 216),\n",
       "             ('[unused212]', 217),\n",
       "             ('[unused213]', 218),\n",
       "             ('[unused214]', 219),\n",
       "             ('[unused215]', 220),\n",
       "             ('[unused216]', 221),\n",
       "             ('[unused217]', 222),\n",
       "             ('[unused218]', 223),\n",
       "             ('[unused219]', 224),\n",
       "             ('[unused220]', 225),\n",
       "             ('[unused221]', 226),\n",
       "             ('[unused222]', 227),\n",
       "             ('[unused223]', 228),\n",
       "             ('[unused224]', 229),\n",
       "             ('[unused225]', 230),\n",
       "             ('[unused226]', 231),\n",
       "             ('[unused227]', 232),\n",
       "             ('[unused228]', 233),\n",
       "             ('[unused229]', 234),\n",
       "             ('[unused230]', 235),\n",
       "             ('[unused231]', 236),\n",
       "             ('[unused232]', 237),\n",
       "             ('[unused233]', 238),\n",
       "             ('[unused234]', 239),\n",
       "             ('[unused235]', 240),\n",
       "             ('[unused236]', 241),\n",
       "             ('[unused237]', 242),\n",
       "             ('[unused238]', 243),\n",
       "             ('[unused239]', 244),\n",
       "             ('[unused240]', 245),\n",
       "             ('[unused241]', 246),\n",
       "             ('[unused242]', 247),\n",
       "             ('[unused243]', 248),\n",
       "             ('[unused244]', 249),\n",
       "             ('[unused245]', 250),\n",
       "             ('[unused246]', 251),\n",
       "             ('[unused247]', 252),\n",
       "             ('[unused248]', 253),\n",
       "             ('[unused249]', 254),\n",
       "             ('[unused250]', 255),\n",
       "             ('[unused251]', 256),\n",
       "             ('[unused252]', 257),\n",
       "             ('[unused253]', 258),\n",
       "             ('[unused254]', 259),\n",
       "             ('[unused255]', 260),\n",
       "             ('[unused256]', 261),\n",
       "             ('[unused257]', 262),\n",
       "             ('[unused258]', 263),\n",
       "             ('[unused259]', 264),\n",
       "             ('[unused260]', 265),\n",
       "             ('[unused261]', 266),\n",
       "             ('[unused262]', 267),\n",
       "             ('[unused263]', 268),\n",
       "             ('[unused264]', 269),\n",
       "             ('[unused265]', 270),\n",
       "             ('[unused266]', 271),\n",
       "             ('[unused267]', 272),\n",
       "             ('[unused268]', 273),\n",
       "             ('[unused269]', 274),\n",
       "             ('[unused270]', 275),\n",
       "             ('[unused271]', 276),\n",
       "             ('[unused272]', 277),\n",
       "             ('[unused273]', 278),\n",
       "             ('[unused274]', 279),\n",
       "             ('[unused275]', 280),\n",
       "             ('[unused276]', 281),\n",
       "             ('[unused277]', 282),\n",
       "             ('[unused278]', 283),\n",
       "             ('[unused279]', 284),\n",
       "             ('[unused280]', 285),\n",
       "             ('[unused281]', 286),\n",
       "             ('[unused282]', 287),\n",
       "             ('[unused283]', 288),\n",
       "             ('[unused284]', 289),\n",
       "             ('[unused285]', 290),\n",
       "             ('[unused286]', 291),\n",
       "             ('[unused287]', 292),\n",
       "             ('[unused288]', 293),\n",
       "             ('[unused289]', 294),\n",
       "             ('[unused290]', 295),\n",
       "             ('[unused291]', 296),\n",
       "             ('[unused292]', 297),\n",
       "             ('[unused293]', 298),\n",
       "             ('[unused294]', 299),\n",
       "             ('[unused295]', 300),\n",
       "             ('[unused296]', 301),\n",
       "             ('[unused297]', 302),\n",
       "             ('[unused298]', 303),\n",
       "             ('[unused299]', 304),\n",
       "             ('[unused300]', 305),\n",
       "             ('[unused301]', 306),\n",
       "             ('[unused302]', 307),\n",
       "             ('[unused303]', 308),\n",
       "             ('[unused304]', 309),\n",
       "             ('[unused305]', 310),\n",
       "             ('[unused306]', 311),\n",
       "             ('[unused307]', 312),\n",
       "             ('[unused308]', 313),\n",
       "             ('[unused309]', 314),\n",
       "             ('[unused310]', 315),\n",
       "             ('[unused311]', 316),\n",
       "             ('[unused312]', 317),\n",
       "             ('[unused313]', 318),\n",
       "             ('[unused314]', 319),\n",
       "             ('[unused315]', 320),\n",
       "             ('[unused316]', 321),\n",
       "             ('[unused317]', 322),\n",
       "             ('[unused318]', 323),\n",
       "             ('[unused319]', 324),\n",
       "             ('[unused320]', 325),\n",
       "             ('[unused321]', 326),\n",
       "             ('[unused322]', 327),\n",
       "             ('[unused323]', 328),\n",
       "             ('[unused324]', 329),\n",
       "             ('[unused325]', 330),\n",
       "             ('[unused326]', 331),\n",
       "             ('[unused327]', 332),\n",
       "             ('[unused328]', 333),\n",
       "             ('[unused329]', 334),\n",
       "             ('[unused330]', 335),\n",
       "             ('[unused331]', 336),\n",
       "             ('[unused332]', 337),\n",
       "             ('[unused333]', 338),\n",
       "             ('[unused334]', 339),\n",
       "             ('[unused335]', 340),\n",
       "             ('[unused336]', 341),\n",
       "             ('[unused337]', 342),\n",
       "             ('[unused338]', 343),\n",
       "             ('[unused339]', 344),\n",
       "             ('[unused340]', 345),\n",
       "             ('[unused341]', 346),\n",
       "             ('[unused342]', 347),\n",
       "             ('[unused343]', 348),\n",
       "             ('[unused344]', 349),\n",
       "             ('[unused345]', 350),\n",
       "             ('[unused346]', 351),\n",
       "             ('[unused347]', 352),\n",
       "             ('[unused348]', 353),\n",
       "             ('[unused349]', 354),\n",
       "             ('[unused350]', 355),\n",
       "             ('[unused351]', 356),\n",
       "             ('[unused352]', 357),\n",
       "             ('[unused353]', 358),\n",
       "             ('[unused354]', 359),\n",
       "             ('[unused355]', 360),\n",
       "             ('[unused356]', 361),\n",
       "             ('[unused357]', 362),\n",
       "             ('[unused358]', 363),\n",
       "             ('[unused359]', 364),\n",
       "             ('[unused360]', 365),\n",
       "             ('[unused361]', 366),\n",
       "             ('[unused362]', 367),\n",
       "             ('[unused363]', 368),\n",
       "             ('[unused364]', 369),\n",
       "             ('[unused365]', 370),\n",
       "             ('[unused366]', 371),\n",
       "             ('[unused367]', 372),\n",
       "             ('[unused368]', 373),\n",
       "             ('[unused369]', 374),\n",
       "             ('[unused370]', 375),\n",
       "             ('[unused371]', 376),\n",
       "             ('[unused372]', 377),\n",
       "             ('[unused373]', 378),\n",
       "             ('[unused374]', 379),\n",
       "             ('[unused375]', 380),\n",
       "             ('[unused376]', 381),\n",
       "             ('[unused377]', 382),\n",
       "             ('[unused378]', 383),\n",
       "             ('[unused379]', 384),\n",
       "             ('[unused380]', 385),\n",
       "             ('[unused381]', 386),\n",
       "             ('[unused382]', 387),\n",
       "             ('[unused383]', 388),\n",
       "             ('[unused384]', 389),\n",
       "             ('[unused385]', 390),\n",
       "             ('[unused386]', 391),\n",
       "             ('[unused387]', 392),\n",
       "             ('[unused388]', 393),\n",
       "             ('[unused389]', 394),\n",
       "             ('[unused390]', 395),\n",
       "             ('[unused391]', 396),\n",
       "             ('[unused392]', 397),\n",
       "             ('[unused393]', 398),\n",
       "             ('[unused394]', 399),\n",
       "             ('[unused395]', 400),\n",
       "             ('[unused396]', 401),\n",
       "             ('[unused397]', 402),\n",
       "             ('[unused398]', 403),\n",
       "             ('[unused399]', 404),\n",
       "             ('[unused400]', 405),\n",
       "             ('[unused401]', 406),\n",
       "             ('[unused402]', 407),\n",
       "             ('[unused403]', 408),\n",
       "             ('[unused404]', 409),\n",
       "             ('[unused405]', 410),\n",
       "             ('[unused406]', 411),\n",
       "             ('[unused407]', 412),\n",
       "             ('[unused408]', 413),\n",
       "             ('[unused409]', 414),\n",
       "             ('[unused410]', 415),\n",
       "             ('[unused411]', 416),\n",
       "             ('[unused412]', 417),\n",
       "             ('[unused413]', 418),\n",
       "             ('[unused414]', 419),\n",
       "             ('[unused415]', 420),\n",
       "             ('[unused416]', 421),\n",
       "             ('[unused417]', 422),\n",
       "             ('[unused418]', 423),\n",
       "             ('[unused419]', 424),\n",
       "             ('[unused420]', 425),\n",
       "             ('[unused421]', 426),\n",
       "             ('[unused422]', 427),\n",
       "             ('[unused423]', 428),\n",
       "             ('[unused424]', 429),\n",
       "             ('[unused425]', 430),\n",
       "             ('[unused426]', 431),\n",
       "             ('[unused427]', 432),\n",
       "             ('[unused428]', 433),\n",
       "             ('[unused429]', 434),\n",
       "             ('[unused430]', 435),\n",
       "             ('[unused431]', 436),\n",
       "             ('[unused432]', 437),\n",
       "             ('[unused433]', 438),\n",
       "             ('[unused434]', 439),\n",
       "             ('[unused435]', 440),\n",
       "             ('[unused436]', 441),\n",
       "             ('[unused437]', 442),\n",
       "             ('[unused438]', 443),\n",
       "             ('[unused439]', 444),\n",
       "             ('[unused440]', 445),\n",
       "             ('[unused441]', 446),\n",
       "             ('[unused442]', 447),\n",
       "             ('[unused443]', 448),\n",
       "             ('[unused444]', 449),\n",
       "             ('[unused445]', 450),\n",
       "             ('[unused446]', 451),\n",
       "             ('[unused447]', 452),\n",
       "             ('[unused448]', 453),\n",
       "             ('[unused449]', 454),\n",
       "             ('[unused450]', 455),\n",
       "             ('[unused451]', 456),\n",
       "             ('[unused452]', 457),\n",
       "             ('[unused453]', 458),\n",
       "             ('[unused454]', 459),\n",
       "             ('[unused455]', 460),\n",
       "             ('[unused456]', 461),\n",
       "             ('[unused457]', 462),\n",
       "             ('[unused458]', 463),\n",
       "             ('[unused459]', 464),\n",
       "             ('[unused460]', 465),\n",
       "             ('[unused461]', 466),\n",
       "             ('[unused462]', 467),\n",
       "             ('[unused463]', 468),\n",
       "             ('[unused464]', 469),\n",
       "             ('[unused465]', 470),\n",
       "             ('[unused466]', 471),\n",
       "             ('[unused467]', 472),\n",
       "             ('[unused468]', 473),\n",
       "             ('[unused469]', 474),\n",
       "             ('[unused470]', 475),\n",
       "             ('[unused471]', 476),\n",
       "             ('[unused472]', 477),\n",
       "             ('[unused473]', 478),\n",
       "             ('[unused474]', 479),\n",
       "             ('[unused475]', 480),\n",
       "             ('[unused476]', 481),\n",
       "             ('[unused477]', 482),\n",
       "             ('[unused478]', 483),\n",
       "             ('[unused479]', 484),\n",
       "             ('[unused480]', 485),\n",
       "             ('[unused481]', 486),\n",
       "             ('[unused482]', 487),\n",
       "             ('[unused483]', 488),\n",
       "             ('[unused484]', 489),\n",
       "             ('[unused485]', 490),\n",
       "             ('[unused486]', 491),\n",
       "             ('[unused487]', 492),\n",
       "             ('[unused488]', 493),\n",
       "             ('[unused489]', 494),\n",
       "             ('[unused490]', 495),\n",
       "             ('[unused491]', 496),\n",
       "             ('[unused492]', 497),\n",
       "             ('[unused493]', 498),\n",
       "             ('[unused494]', 499),\n",
       "             ('[unused495]', 500),\n",
       "             ('[unused496]', 501),\n",
       "             ('[unused497]', 502),\n",
       "             ('[unused498]', 503),\n",
       "             ('[unused499]', 504),\n",
       "             ('[unused500]', 505),\n",
       "             ('[unused501]', 506),\n",
       "             ('[unused502]', 507),\n",
       "             ('[unused503]', 508),\n",
       "             ('[unused504]', 509),\n",
       "             ('[unused505]', 510),\n",
       "             ('[unused506]', 511),\n",
       "             ('[unused507]', 512),\n",
       "             ('[unused508]', 513),\n",
       "             ('[unused509]', 514),\n",
       "             ('[unused510]', 515),\n",
       "             ('[unused511]', 516),\n",
       "             ('[unused512]', 517),\n",
       "             ('[unused513]', 518),\n",
       "             ('[unused514]', 519),\n",
       "             ('[unused515]', 520),\n",
       "             ('[unused516]', 521),\n",
       "             ('[unused517]', 522),\n",
       "             ('[unused518]', 523),\n",
       "             ('[unused519]', 524),\n",
       "             ('[unused520]', 525),\n",
       "             ('[unused521]', 526),\n",
       "             ('[unused522]', 527),\n",
       "             ('[unused523]', 528),\n",
       "             ('[unused524]', 529),\n",
       "             ('[unused525]', 530),\n",
       "             ('[unused526]', 531),\n",
       "             ('[unused527]', 532),\n",
       "             ('[unused528]', 533),\n",
       "             ('[unused529]', 534),\n",
       "             ('[unused530]', 535),\n",
       "             ('[unused531]', 536),\n",
       "             ('[unused532]', 537),\n",
       "             ('[unused533]', 538),\n",
       "             ('[unused534]', 539),\n",
       "             ('[unused535]', 540),\n",
       "             ('[unused536]', 541),\n",
       "             ('[unused537]', 542),\n",
       "             ('[unused538]', 543),\n",
       "             ('[unused539]', 544),\n",
       "             ('[unused540]', 545),\n",
       "             ('[unused541]', 546),\n",
       "             ('[unused542]', 547),\n",
       "             ('[unused543]', 548),\n",
       "             ('[unused544]', 549),\n",
       "             ('[unused545]', 550),\n",
       "             ('[unused546]', 551),\n",
       "             ('[unused547]', 552),\n",
       "             ('[unused548]', 553),\n",
       "             ('[unused549]', 554),\n",
       "             ('[unused550]', 555),\n",
       "             ('[unused551]', 556),\n",
       "             ('[unused552]', 557),\n",
       "             ('[unused553]', 558),\n",
       "             ('[unused554]', 559),\n",
       "             ('[unused555]', 560),\n",
       "             ('[unused556]', 561),\n",
       "             ('[unused557]', 562),\n",
       "             ('[unused558]', 563),\n",
       "             ('[unused559]', 564),\n",
       "             ('[unused560]', 565),\n",
       "             ('[unused561]', 566),\n",
       "             ('[unused562]', 567),\n",
       "             ('[unused563]', 568),\n",
       "             ('[unused564]', 569),\n",
       "             ('[unused565]', 570),\n",
       "             ('[unused566]', 571),\n",
       "             ('[unused567]', 572),\n",
       "             ('[unused568]', 573),\n",
       "             ('[unused569]', 574),\n",
       "             ('[unused570]', 575),\n",
       "             ('[unused571]', 576),\n",
       "             ('[unused572]', 577),\n",
       "             ('[unused573]', 578),\n",
       "             ('[unused574]', 579),\n",
       "             ('[unused575]', 580),\n",
       "             ('[unused576]', 581),\n",
       "             ('[unused577]', 582),\n",
       "             ('[unused578]', 583),\n",
       "             ('[unused579]', 584),\n",
       "             ('[unused580]', 585),\n",
       "             ('[unused581]', 586),\n",
       "             ('[unused582]', 587),\n",
       "             ('[unused583]', 588),\n",
       "             ('[unused584]', 589),\n",
       "             ('[unused585]', 590),\n",
       "             ('[unused586]', 591),\n",
       "             ('[unused587]', 592),\n",
       "             ('[unused588]', 593),\n",
       "             ('[unused589]', 594),\n",
       "             ('[unused590]', 595),\n",
       "             ('[unused591]', 596),\n",
       "             ('[unused592]', 597),\n",
       "             ('[unused593]', 598),\n",
       "             ('[unused594]', 599),\n",
       "             ('[unused595]', 600),\n",
       "             ('[unused596]', 601),\n",
       "             ('[unused597]', 602),\n",
       "             ('[unused598]', 603),\n",
       "             ('[unused599]', 604),\n",
       "             ('[unused600]', 605),\n",
       "             ('[unused601]', 606),\n",
       "             ('[unused602]', 607),\n",
       "             ('[unused603]', 608),\n",
       "             ('[unused604]', 609),\n",
       "             ('[unused605]', 610),\n",
       "             ('[unused606]', 611),\n",
       "             ('[unused607]', 612),\n",
       "             ('[unused608]', 613),\n",
       "             ('[unused609]', 614),\n",
       "             ('[unused610]', 615),\n",
       "             ('[unused611]', 616),\n",
       "             ('[unused612]', 617),\n",
       "             ('[unused613]', 618),\n",
       "             ('[unused614]', 619),\n",
       "             ('[unused615]', 620),\n",
       "             ('[unused616]', 621),\n",
       "             ('[unused617]', 622),\n",
       "             ('[unused618]', 623),\n",
       "             ('[unused619]', 624),\n",
       "             ('[unused620]', 625),\n",
       "             ('[unused621]', 626),\n",
       "             ('[unused622]', 627),\n",
       "             ('[unused623]', 628),\n",
       "             ('[unused624]', 629),\n",
       "             ('[unused625]', 630),\n",
       "             ('[unused626]', 631),\n",
       "             ('[unused627]', 632),\n",
       "             ('[unused628]', 633),\n",
       "             ('[unused629]', 634),\n",
       "             ('[unused630]', 635),\n",
       "             ('[unused631]', 636),\n",
       "             ('[unused632]', 637),\n",
       "             ('[unused633]', 638),\n",
       "             ('[unused634]', 639),\n",
       "             ('[unused635]', 640),\n",
       "             ('[unused636]', 641),\n",
       "             ('[unused637]', 642),\n",
       "             ('[unused638]', 643),\n",
       "             ('[unused639]', 644),\n",
       "             ('[unused640]', 645),\n",
       "             ('[unused641]', 646),\n",
       "             ('[unused642]', 647),\n",
       "             ('[unused643]', 648),\n",
       "             ('[unused644]', 649),\n",
       "             ('[unused645]', 650),\n",
       "             ('[unused646]', 651),\n",
       "             ('[unused647]', 652),\n",
       "             ('[unused648]', 653),\n",
       "             ('[unused649]', 654),\n",
       "             ('[unused650]', 655),\n",
       "             ('[unused651]', 656),\n",
       "             ('[unused652]', 657),\n",
       "             ('[unused653]', 658),\n",
       "             ('[unused654]', 659),\n",
       "             ('[unused655]', 660),\n",
       "             ('[unused656]', 661),\n",
       "             ('[unused657]', 662),\n",
       "             ('[unused658]', 663),\n",
       "             ('[unused659]', 664),\n",
       "             ('[unused660]', 665),\n",
       "             ('[unused661]', 666),\n",
       "             ('[unused662]', 667),\n",
       "             ('[unused663]', 668),\n",
       "             ('[unused664]', 669),\n",
       "             ('[unused665]', 670),\n",
       "             ('[unused666]', 671),\n",
       "             ('[unused667]', 672),\n",
       "             ('[unused668]', 673),\n",
       "             ('[unused669]', 674),\n",
       "             ('[unused670]', 675),\n",
       "             ('[unused671]', 676),\n",
       "             ('[unused672]', 677),\n",
       "             ('[unused673]', 678),\n",
       "             ('[unused674]', 679),\n",
       "             ('[unused675]', 680),\n",
       "             ('[unused676]', 681),\n",
       "             ('[unused677]', 682),\n",
       "             ('[unused678]', 683),\n",
       "             ('[unused679]', 684),\n",
       "             ('[unused680]', 685),\n",
       "             ('[unused681]', 686),\n",
       "             ('[unused682]', 687),\n",
       "             ('[unused683]', 688),\n",
       "             ('[unused684]', 689),\n",
       "             ('[unused685]', 690),\n",
       "             ('[unused686]', 691),\n",
       "             ('[unused687]', 692),\n",
       "             ('[unused688]', 693),\n",
       "             ('[unused689]', 694),\n",
       "             ('[unused690]', 695),\n",
       "             ('[unused691]', 696),\n",
       "             ('[unused692]', 697),\n",
       "             ('[unused693]', 698),\n",
       "             ('[unused694]', 699),\n",
       "             ('[unused695]', 700),\n",
       "             ('[unused696]', 701),\n",
       "             ('[unused697]', 702),\n",
       "             ('[unused698]', 703),\n",
       "             ('[unused699]', 704),\n",
       "             ('[unused700]', 705),\n",
       "             ('[unused701]', 706),\n",
       "             ('[unused702]', 707),\n",
       "             ('[unused703]', 708),\n",
       "             ('[unused704]', 709),\n",
       "             ('[unused705]', 710),\n",
       "             ('[unused706]', 711),\n",
       "             ('[unused707]', 712),\n",
       "             ('[unused708]', 713),\n",
       "             ('[unused709]', 714),\n",
       "             ('[unused710]', 715),\n",
       "             ('[unused711]', 716),\n",
       "             ('[unused712]', 717),\n",
       "             ('[unused713]', 718),\n",
       "             ('[unused714]', 719),\n",
       "             ('[unused715]', 720),\n",
       "             ('[unused716]', 721),\n",
       "             ('[unused717]', 722),\n",
       "             ('[unused718]', 723),\n",
       "             ('[unused719]', 724),\n",
       "             ('[unused720]', 725),\n",
       "             ('[unused721]', 726),\n",
       "             ('[unused722]', 727),\n",
       "             ('[unused723]', 728),\n",
       "             ('[unused724]', 729),\n",
       "             ('[unused725]', 730),\n",
       "             ('[unused726]', 731),\n",
       "             ('[unused727]', 732),\n",
       "             ('[unused728]', 733),\n",
       "             ('[unused729]', 734),\n",
       "             ('[unused730]', 735),\n",
       "             ('[unused731]', 736),\n",
       "             ('[unused732]', 737),\n",
       "             ('[unused733]', 738),\n",
       "             ('[unused734]', 739),\n",
       "             ('[unused735]', 740),\n",
       "             ('[unused736]', 741),\n",
       "             ('[unused737]', 742),\n",
       "             ('[unused738]', 743),\n",
       "             ('[unused739]', 744),\n",
       "             ('[unused740]', 745),\n",
       "             ('[unused741]', 746),\n",
       "             ('[unused742]', 747),\n",
       "             ('[unused743]', 748),\n",
       "             ('[unused744]', 749),\n",
       "             ('[unused745]', 750),\n",
       "             ('[unused746]', 751),\n",
       "             ('[unused747]', 752),\n",
       "             ('[unused748]', 753),\n",
       "             ('[unused749]', 754),\n",
       "             ('[unused750]', 755),\n",
       "             ('[unused751]', 756),\n",
       "             ('[unused752]', 757),\n",
       "             ('[unused753]', 758),\n",
       "             ('[unused754]', 759),\n",
       "             ('[unused755]', 760),\n",
       "             ('[unused756]', 761),\n",
       "             ('[unused757]', 762),\n",
       "             ('[unused758]', 763),\n",
       "             ('[unused759]', 764),\n",
       "             ('[unused760]', 765),\n",
       "             ('[unused761]', 766),\n",
       "             ('[unused762]', 767),\n",
       "             ('[unused763]', 768),\n",
       "             ('[unused764]', 769),\n",
       "             ('[unused765]', 770),\n",
       "             ('[unused766]', 771),\n",
       "             ('[unused767]', 772),\n",
       "             ('[unused768]', 773),\n",
       "             ('[unused769]', 774),\n",
       "             ('[unused770]', 775),\n",
       "             ('[unused771]', 776),\n",
       "             ('[unused772]', 777),\n",
       "             ('[unused773]', 778),\n",
       "             ('[unused774]', 779),\n",
       "             ('[unused775]', 780),\n",
       "             ('[unused776]', 781),\n",
       "             ('[unused777]', 782),\n",
       "             ('[unused778]', 783),\n",
       "             ('[unused779]', 784),\n",
       "             ('[unused780]', 785),\n",
       "             ('[unused781]', 786),\n",
       "             ('[unused782]', 787),\n",
       "             ('[unused783]', 788),\n",
       "             ('[unused784]', 789),\n",
       "             ('[unused785]', 790),\n",
       "             ('[unused786]', 791),\n",
       "             ('[unused787]', 792),\n",
       "             ('[unused788]', 793),\n",
       "             ('[unused789]', 794),\n",
       "             ('[unused790]', 795),\n",
       "             ('[unused791]', 796),\n",
       "             ('[unused792]', 797),\n",
       "             ('[unused793]', 798),\n",
       "             ('[unused794]', 799),\n",
       "             ('[unused795]', 800),\n",
       "             ('[unused796]', 801),\n",
       "             ('[unused797]', 802),\n",
       "             ('[unused798]', 803),\n",
       "             ('[unused799]', 804),\n",
       "             ('[unused800]', 805),\n",
       "             ('[unused801]', 806),\n",
       "             ('[unused802]', 807),\n",
       "             ('[unused803]', 808),\n",
       "             ('[unused804]', 809),\n",
       "             ('[unused805]', 810),\n",
       "             ('[unused806]', 811),\n",
       "             ('[unused807]', 812),\n",
       "             ('[unused808]', 813),\n",
       "             ('[unused809]', 814),\n",
       "             ('[unused810]', 815),\n",
       "             ('[unused811]', 816),\n",
       "             ('[unused812]', 817),\n",
       "             ('[unused813]', 818),\n",
       "             ('[unused814]', 819),\n",
       "             ('[unused815]', 820),\n",
       "             ('[unused816]', 821),\n",
       "             ('[unused817]', 822),\n",
       "             ('[unused818]', 823),\n",
       "             ('[unused819]', 824),\n",
       "             ('[unused820]', 825),\n",
       "             ('[unused821]', 826),\n",
       "             ('[unused822]', 827),\n",
       "             ('[unused823]', 828),\n",
       "             ('[unused824]', 829),\n",
       "             ('[unused825]', 830),\n",
       "             ('[unused826]', 831),\n",
       "             ('[unused827]', 832),\n",
       "             ('[unused828]', 833),\n",
       "             ('[unused829]', 834),\n",
       "             ('[unused830]', 835),\n",
       "             ('[unused831]', 836),\n",
       "             ('[unused832]', 837),\n",
       "             ('[unused833]', 838),\n",
       "             ('[unused834]', 839),\n",
       "             ('[unused835]', 840),\n",
       "             ('[unused836]', 841),\n",
       "             ('[unused837]', 842),\n",
       "             ('[unused838]', 843),\n",
       "             ('[unused839]', 844),\n",
       "             ('[unused840]', 845),\n",
       "             ('[unused841]', 846),\n",
       "             ('[unused842]', 847),\n",
       "             ('[unused843]', 848),\n",
       "             ('[unused844]', 849),\n",
       "             ('[unused845]', 850),\n",
       "             ('[unused846]', 851),\n",
       "             ('[unused847]', 852),\n",
       "             ('[unused848]', 853),\n",
       "             ('[unused849]', 854),\n",
       "             ('[unused850]', 855),\n",
       "             ('[unused851]', 856),\n",
       "             ('[unused852]', 857),\n",
       "             ('[unused853]', 858),\n",
       "             ('[unused854]', 859),\n",
       "             ('[unused855]', 860),\n",
       "             ('[unused856]', 861),\n",
       "             ('[unused857]', 862),\n",
       "             ('[unused858]', 863),\n",
       "             ('[unused859]', 864),\n",
       "             ('[unused860]', 865),\n",
       "             ('[unused861]', 866),\n",
       "             ('[unused862]', 867),\n",
       "             ('[unused863]', 868),\n",
       "             ('[unused864]', 869),\n",
       "             ('[unused865]', 870),\n",
       "             ('[unused866]', 871),\n",
       "             ('[unused867]', 872),\n",
       "             ('[unused868]', 873),\n",
       "             ('[unused869]', 874),\n",
       "             ('[unused870]', 875),\n",
       "             ('[unused871]', 876),\n",
       "             ('[unused872]', 877),\n",
       "             ('[unused873]', 878),\n",
       "             ('[unused874]', 879),\n",
       "             ('[unused875]', 880),\n",
       "             ('[unused876]', 881),\n",
       "             ('[unused877]', 882),\n",
       "             ('[unused878]', 883),\n",
       "             ('[unused879]', 884),\n",
       "             ('[unused880]', 885),\n",
       "             ('[unused881]', 886),\n",
       "             ('[unused882]', 887),\n",
       "             ('[unused883]', 888),\n",
       "             ('[unused884]', 889),\n",
       "             ('[unused885]', 890),\n",
       "             ('[unused886]', 891),\n",
       "             ('[unused887]', 892),\n",
       "             ('[unused888]', 893),\n",
       "             ('[unused889]', 894),\n",
       "             ('[unused890]', 895),\n",
       "             ('[unused891]', 896),\n",
       "             ('[unused892]', 897),\n",
       "             ('[unused893]', 898),\n",
       "             ('[unused894]', 899),\n",
       "             ('[unused895]', 900),\n",
       "             ('[unused896]', 901),\n",
       "             ('[unused897]', 902),\n",
       "             ('[unused898]', 903),\n",
       "             ('[unused899]', 904),\n",
       "             ('[unused900]', 905),\n",
       "             ('[unused901]', 906),\n",
       "             ('[unused902]', 907),\n",
       "             ('[unused903]', 908),\n",
       "             ('[unused904]', 909),\n",
       "             ('[unused905]', 910),\n",
       "             ('[unused906]', 911),\n",
       "             ('[unused907]', 912),\n",
       "             ('[unused908]', 913),\n",
       "             ('[unused909]', 914),\n",
       "             ('[unused910]', 915),\n",
       "             ('[unused911]', 916),\n",
       "             ('[unused912]', 917),\n",
       "             ('[unused913]', 918),\n",
       "             ('[unused914]', 919),\n",
       "             ('[unused915]', 920),\n",
       "             ('[unused916]', 921),\n",
       "             ('[unused917]', 922),\n",
       "             ('[unused918]', 923),\n",
       "             ('[unused919]', 924),\n",
       "             ('[unused920]', 925),\n",
       "             ('[unused921]', 926),\n",
       "             ('[unused922]', 927),\n",
       "             ('[unused923]', 928),\n",
       "             ('[unused924]', 929),\n",
       "             ('[unused925]', 930),\n",
       "             ('[unused926]', 931),\n",
       "             ('[unused927]', 932),\n",
       "             ('[unused928]', 933),\n",
       "             ('[unused929]', 934),\n",
       "             ('[unused930]', 935),\n",
       "             ('[unused931]', 936),\n",
       "             ('[unused932]', 937),\n",
       "             ('[unused933]', 938),\n",
       "             ('[unused934]', 939),\n",
       "             ('[unused935]', 940),\n",
       "             ('[unused936]', 941),\n",
       "             ('[unused937]', 942),\n",
       "             ('[unused938]', 943),\n",
       "             ('[unused939]', 944),\n",
       "             ('[unused940]', 945),\n",
       "             ('[unused941]', 946),\n",
       "             ('[unused942]', 947),\n",
       "             ('[unused943]', 948),\n",
       "             ('[unused944]', 949),\n",
       "             ('[unused945]', 950),\n",
       "             ('[unused946]', 951),\n",
       "             ('[unused947]', 952),\n",
       "             ('[unused948]', 953),\n",
       "             ('[unused949]', 954),\n",
       "             ('[unused950]', 955),\n",
       "             ('[unused951]', 956),\n",
       "             ('[unused952]', 957),\n",
       "             ('[unused953]', 958),\n",
       "             ('[unused954]', 959),\n",
       "             ('[unused955]', 960),\n",
       "             ('[unused956]', 961),\n",
       "             ('[unused957]', 962),\n",
       "             ('[unused958]', 963),\n",
       "             ('[unused959]', 964),\n",
       "             ('[unused960]', 965),\n",
       "             ('[unused961]', 966),\n",
       "             ('[unused962]', 967),\n",
       "             ('[unused963]', 968),\n",
       "             ('[unused964]', 969),\n",
       "             ('[unused965]', 970),\n",
       "             ('[unused966]', 971),\n",
       "             ('[unused967]', 972),\n",
       "             ('[unused968]', 973),\n",
       "             ('[unused969]', 974),\n",
       "             ('[unused970]', 975),\n",
       "             ('[unused971]', 976),\n",
       "             ('[unused972]', 977),\n",
       "             ('[unused973]', 978),\n",
       "             ('[unused974]', 979),\n",
       "             ('[unused975]', 980),\n",
       "             ('[unused976]', 981),\n",
       "             ('[unused977]', 982),\n",
       "             ('[unused978]', 983),\n",
       "             ('[unused979]', 984),\n",
       "             ('[unused980]', 985),\n",
       "             ('[unused981]', 986),\n",
       "             ('[unused982]', 987),\n",
       "             ('[unused983]', 988),\n",
       "             ('[unused984]', 989),\n",
       "             ('[unused985]', 990),\n",
       "             ('[unused986]', 991),\n",
       "             ('[unused987]', 992),\n",
       "             ('[unused988]', 993),\n",
       "             ('[unused989]', 994),\n",
       "             ('[unused990]', 995),\n",
       "             ('[unused991]', 996),\n",
       "             ('[unused992]', 997),\n",
       "             ('[unused993]', 998),\n",
       "             ('!', 999),\n",
       "             ...])"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "3lV54Y6GV1dR"
   },
   "outputs": [],
   "source": [
    "def convert_to_unicode(text):\n",
    "    if isinstance(text, str):\n",
    "        return text\n",
    "    elif isinstance(text, bytes):\n",
    "        return text.decode(\"utf-8\", \"ignore\")\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported string type: %s\" % (type(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "dxtrgygQ9dnZ"
   },
   "outputs": [],
   "source": [
    "def create_tokens(text):\n",
    "  text = convert_to_unicode(text)\n",
    "  text = text.strip()\n",
    "  text = text.lower()\n",
    "  if not text:\n",
    "    tokens=[]\n",
    "  else:  \n",
    "    tokens = text.split()\n",
    "  return tokens   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "jxXWDYphEERI"
   },
   "outputs": [],
   "source": [
    "# tokenize('  he  llo123   ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "PYYp-MzVEJou"
   },
   "outputs": [],
   "source": [
    "def vocab_tokenize(tokens,vocab,unk_token=\"[UNK]\"):\n",
    "    output_tokens = []\n",
    "    for token in tokens:\n",
    "        chars = list(token)\n",
    "        is_bad = False\n",
    "        start = 0\n",
    "        sub_tokens = []\n",
    "        while start < len(chars):\n",
    "            end = len(chars)\n",
    "            cur_substr = None\n",
    "            while start < end:\n",
    "\n",
    "                substr = \"\".join(chars[start:end])\n",
    "                if start > 0:\n",
    "                    substr = \"##\" + substr \n",
    "                    \n",
    "                if substr in vocab:\n",
    "                    cur_substr = substr\n",
    "                    break\n",
    "                end -= 1\n",
    "            if cur_substr is None:\n",
    "                is_bad = True\n",
    "                break\n",
    "            sub_tokens.append(cur_substr)\n",
    "            start = end\n",
    "        if is_bad:\n",
    "            output_tokens.append(unk_token)\n",
    "        else:\n",
    "            output_tokens.extend(sub_tokens)\n",
    "    return output_tokens    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ujRuwDk6I1Pe",
    "outputId": "eb353fa8-829a-4acb-e141-bb0bad80c3dd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['una', '##ffa', '##ble']"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_tokenize(['unaffable'],vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "2gmb5hmMI6sK"
   },
   "outputs": [],
   "source": [
    "def tokenizer(text,vocab=vocab):\n",
    "  tokens = create_tokens(text)\n",
    "  # print(tokens)\n",
    "  return vocab_tokenize(tokens,vocab)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "CYyn1c4wPDCM"
   },
   "outputs": [],
   "source": [
    "text = 'What is the score of the event with a competition type of friendly and a result of 4-5?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UCsm7rnbO1oX",
    "outputId": "8050198b-2c1f-44b7-c55f-50f756437090"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['what',\n",
       " 'is',\n",
       " 'the',\n",
       " 'score',\n",
       " 'of',\n",
       " 'the',\n",
       " 'event',\n",
       " 'with',\n",
       " 'a',\n",
       " 'competition',\n",
       " 'type',\n",
       " 'of',\n",
       " 'friendly',\n",
       " 'and',\n",
       " 'a',\n",
       " 'result',\n",
       " 'of',\n",
       " '4',\n",
       " '##-',\n",
       " '##5',\n",
       " '##?']"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "pWqItCTvPNLC"
   },
   "outputs": [],
   "source": [
    "def convert_tokens_to_ids(tokens,vocab=vocab):\n",
    "    ids = []\n",
    "    for token in tokens:\n",
    "        ids.append(vocab[token])\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RqyyruDNbGyo"
   },
   "source": [
    "# BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "xVyKQ8SyMcLa"
   },
   "outputs": [],
   "source": [
    "class BERTLayerNorm(nn.Module):\n",
    "    def __init__(self, hidden_size, variance_epsilon=1e-12):\n",
    "        super(BERTLayerNorm, self).__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(hidden_size))\n",
    "        self.beta = nn.Parameter(torch.zeros(hidden_size))\n",
    "        self.variance_epsilon = variance_epsilon\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        u = x.mean(-1, keepdim=True) \n",
    "        s = (x - u).pow(2).mean(-1, keepdim=True)\n",
    "        x = (x - u) / torch.sqrt(s + self.variance_epsilon) \n",
    "\n",
    "        return self.gamma * x + self.beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "L1pxXRBWXUv-"
   },
   "outputs": [],
   "source": [
    "def gelu(x):\n",
    "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "X0woQYfUj65i"
   },
   "outputs": [],
   "source": [
    "class BertModel(nn.Module):\n",
    "\n",
    "    def __init__(self,vocab_size,hidden_size,max_position_embeddings,type_vocab_size,hidden_dropout_prob,num_hidden_layers,num_attention_heads,attention_probs_dropout_prob,intermediate_size):\n",
    "        super(BertModel, self).__init__()\n",
    "        self.embeddings = BERTEmbeddings(vocab_size,hidden_size,max_position_embeddings,type_vocab_size,hidden_dropout_prob)\n",
    "        self.encoder = BERTEncoder(num_hidden_layers,hidden_size,num_attention_heads,attention_probs_dropout_prob,hidden_dropout_prob,intermediate_size)\n",
    "        self.pooler = BERTPooler(hidden_size)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None):\n",
    "        if attention_mask is None:\n",
    "            attention_mask = torch.ones_like(input_ids)\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = torch.zeros_like(input_ids)\n",
    "\n",
    "        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "        extended_attention_mask = extended_attention_mask.float()\n",
    "        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
    "\n",
    "        embedding_output = self.embeddings(input_ids, token_type_ids)\n",
    "        all_encoder_layers = self.encoder(embedding_output, extended_attention_mask)\n",
    "        sequence_output = all_encoder_layers[-1]\n",
    "        pooled_output = self.pooler(sequence_output)\n",
    "        return all_encoder_layers, pooled_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kWR3v7Kv6twW"
   },
   "source": [
    "### BERT Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "NmjKISvkMKwy"
   },
   "outputs": [],
   "source": [
    "class BERTEmbeddings(nn.Module):\n",
    "    def __init__(self,vocab_size,hidden_size,max_position_embeddings,type_vocab_size,hidden_dropout_prob):\n",
    "        super(BERTEmbeddings, self).__init__()\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.position_embeddings = nn.Embedding(max_position_embeddings, hidden_size)\n",
    "        self.token_type_embeddings = nn.Embedding(type_vocab_size, hidden_size)\n",
    "\n",
    "        self.LayerNorm = BERTLayerNorm(hidden_size)\n",
    "        self.dropout = nn.Dropout(hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None):\n",
    "        seq_length = input_ids.size(1)\n",
    "        position_ids = torch.arange(seq_length, dtype=torch.long, device=input_ids.device)\n",
    "        position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = torch.zeros_like(input_ids)\n",
    "\n",
    "        words_embeddings = self.word_embeddings(input_ids)\n",
    "        position_embeddings = self.position_embeddings(position_ids)\n",
    "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
    "\n",
    "        embeddings = words_embeddings + position_embeddings + token_type_embeddings\n",
    "        embeddings = self.LayerNorm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4bPXjJOu6iJR"
   },
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "n7BYnKKEMuym"
   },
   "outputs": [],
   "source": [
    "class BERTEncoder(nn.Module):\n",
    "    def __init__(self, num_hidden_layers,hidden_size,num_attention_heads,attention_probs_dropout_prob,hidden_dropout_prob,intermediate_size):\n",
    "        super(BERTEncoder, self).__init__()\n",
    "        layer = BERTLayer(hidden_size,num_attention_heads,attention_probs_dropout_prob,hidden_dropout_prob,intermediate_size)\n",
    "        self.layer = nn.ModuleList([copy.deepcopy(layer) for _ in range(num_hidden_layers)])    \n",
    "\n",
    "    def forward(self, hidden_states, attention_mask):\n",
    "        all_encoder_layers = []\n",
    "        for layer_module in self.layer:\n",
    "            hidden_states = layer_module(hidden_states, attention_mask)\n",
    "            all_encoder_layers.append(hidden_states)\n",
    "        return all_encoder_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "5lEDAak_rp_y"
   },
   "outputs": [],
   "source": [
    "class BERTLayer(nn.Module):\n",
    "    def __init__(self, hidden_size,num_attention_heads,attention_probs_dropout_prob,hidden_dropout_prob,intermediate_size):\n",
    "        super(BERTLayer, self).__init__()\n",
    "        self.attention = BERTAttention(hidden_size,num_attention_heads,attention_probs_dropout_prob,hidden_dropout_prob)\n",
    "        self.intermediate = BERTIntermediate(hidden_size, intermediate_size)\n",
    "        self.output = BERTOutput(intermediate_size, hidden_size,hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask):\n",
    "        attention_output = self.attention(hidden_states, attention_mask)\n",
    "        # print(attention_output) \n",
    "        # print('att = ',type(attention_output),attention_output.shape)\n",
    "        intermediate_output = self.intermediate(attention_output)\n",
    "        layer_output = self.output(intermediate_output, attention_output)\n",
    "        return layer_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lcRVA7T46Yol"
   },
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "3EFP_JqzruV2"
   },
   "outputs": [],
   "source": [
    "class BERTAttention(nn.Module):\n",
    "    def __init__(self, hidden_size,num_attention_heads,attention_probs_dropout_prob,hidden_dropout_prob):\n",
    "        super(BERTAttention, self).__init__()\n",
    "        self.self = BERTSelfAttention(hidden_size,num_attention_heads,attention_probs_dropout_prob)\n",
    "        self.output = BERTSelfOutput(hidden_size,hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, input_tensor, attention_mask):\n",
    "        self_output = self.self(input_tensor, attention_mask)\n",
    "        attention_output = self.output(self_output, input_tensor)\n",
    "        return attention_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "X50vmNRKr59l"
   },
   "outputs": [],
   "source": [
    "class BERTSelfAttention(nn.Module):\n",
    "    def __init__(self, hidden_size,num_attention_heads,attention_probs_dropout_prob):\n",
    "        super(BERTSelfAttention, self).__init__()\n",
    "        if hidden_size % num_attention_heads != 0:\n",
    "            raise ValueError(\n",
    "                \"The hidden size (%d) is not a multiple of the number of attention \"\n",
    "                \"heads (%d)\" % (hidden_size, num_attention_heads))\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.attention_head_size = int(hidden_size / num_attention_heads)\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "\n",
    "        self.query = nn.Linear(hidden_size, self.all_head_size)\n",
    "        self.key = nn.Linear(hidden_size, self.all_head_size)\n",
    "        self.value = nn.Linear(hidden_size, self.all_head_size)\n",
    "\n",
    "        self.dropout = nn.Dropout(attention_probs_dropout_prob)\n",
    "\n",
    "    def transpose_for_scores(self, x):\n",
    "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)   \n",
    "\n",
    "        x = x.view(*new_x_shape)\n",
    "\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask):\n",
    "        mixed_query_layer = self.query(hidden_states)\n",
    "        mixed_key_layer = self.key(hidden_states)\n",
    "        mixed_value_layer = self.value(hidden_states)\n",
    "\n",
    "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
    "        key_layer = self.transpose_for_scores(mixed_key_layer)\n",
    "        value_layer = self.transpose_for_scores(mixed_value_layer)\n",
    "\n",
    "       \n",
    "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
    "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
    "        \n",
    "        attention_scores = attention_scores + attention_mask \n",
    "\n",
    "        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n",
    "\n",
    "        \n",
    "        attention_probs = self.dropout(attention_probs)\n",
    "\n",
    "       \n",
    "        context_layer = torch.matmul(attention_probs, value_layer)\n",
    "\n",
    "        \n",
    "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "\n",
    "        \n",
    "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
    "        context_layer = context_layer.view(*new_context_layer_shape)\n",
    "        return context_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "d9WY8rGCsftG"
   },
   "outputs": [],
   "source": [
    "class BERTIntermediate(nn.Module):\n",
    "    def __init__(self, hidden_size, intermediate_size):\n",
    "        super(BERTIntermediate, self).__init__()\n",
    "        self.dense = nn.Linear(hidden_size, intermediate_size)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = gelu(hidden_states)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "mLd60jMtsXLa"
   },
   "outputs": [],
   "source": [
    "class BERTSelfOutput(nn.Module):\n",
    "    def __init__(self, hidden_size,hidden_dropout_prob):\n",
    "        super(BERTSelfOutput, self).__init__()\n",
    "        self.dense = nn.Linear(hidden_size, hidden_size)\n",
    "        self.LayerNorm = BERTLayerNorm(hidden_size)\n",
    "        self.dropout = nn.Dropout(hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, hidden_states, input_tensor):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "cK2QB8IPsmwU"
   },
   "outputs": [],
   "source": [
    "class BERTOutput(nn.Module):\n",
    "    def __init__(self, intermediate_size, hidden_size,hidden_dropout_prob):\n",
    "        super(BERTOutput, self).__init__()\n",
    "        self.dense = nn.Linear(intermediate_size, hidden_size)\n",
    "        self.LayerNorm = BERTLayerNorm(hidden_size)\n",
    "        self.dropout = nn.Dropout(hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, hidden_states, input_tensor):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "oyTX30WWMySp"
   },
   "outputs": [],
   "source": [
    "class BERTPooler(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(BERTPooler, self).__init__()\n",
    "        self.dense = nn.Linear(hidden_size, hidden_size)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        \n",
    "        first_token_tensor = hidden_states[:, 0]\n",
    "        pooled_output = self.dense(first_token_tensor)\n",
    "        pooled_output = self.activation(pooled_output)\n",
    "        return pooled_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "duhj1e1lpsQy"
   },
   "outputs": [],
   "source": [
    "attention_probs_dropout_prob = 0.1\n",
    "hidden_act = \"gelu\"\n",
    "hidden_dropout_prob = 0.1\n",
    "hidden_size = 768\n",
    "initializer_range = 0.02\n",
    "intermediate_size = 3072\n",
    "max_position_embeddings = 512\n",
    "num_attention_heads = 12\n",
    "num_hidden_layers = 12\n",
    "type_vocab_size = 2\n",
    "vocab_size = 30522"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "Do-yODxRNm0X"
   },
   "outputs": [],
   "source": [
    "max_seq_length = 250\n",
    "num_target_layers=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "nGnONOnbpN4g"
   },
   "outputs": [],
   "source": [
    "model_bert = BertModel(vocab_size,hidden_size,max_position_embeddings,type_vocab_size,hidden_dropout_prob,num_hidden_layers,num_attention_heads,attention_probs_dropout_prob,intermediate_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4snGwDP9ThV2",
    "outputId": "bf23f80d-357c-4e66-830d-c269ba8eb192"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BERTEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): BERTLayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BERTEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BERTLayer(\n",
       "        (attention): BERTAttention(\n",
       "          (self): BERTSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BERTSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BERTLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BERTIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BERTOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BERTLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BERTLayer(\n",
       "        (attention): BERTAttention(\n",
       "          (self): BERTSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BERTSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BERTLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BERTIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BERTOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BERTLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BERTLayer(\n",
       "        (attention): BERTAttention(\n",
       "          (self): BERTSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BERTSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BERTLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BERTIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BERTOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BERTLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BERTLayer(\n",
       "        (attention): BERTAttention(\n",
       "          (self): BERTSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BERTSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BERTLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BERTIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BERTOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BERTLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BERTLayer(\n",
       "        (attention): BERTAttention(\n",
       "          (self): BERTSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BERTSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BERTLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BERTIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BERTOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BERTLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BERTLayer(\n",
       "        (attention): BERTAttention(\n",
       "          (self): BERTSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BERTSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BERTLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BERTIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BERTOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BERTLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BERTLayer(\n",
       "        (attention): BERTAttention(\n",
       "          (self): BERTSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BERTSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BERTLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BERTIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BERTOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BERTLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BERTLayer(\n",
       "        (attention): BERTAttention(\n",
       "          (self): BERTSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BERTSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BERTLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BERTIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BERTOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BERTLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BERTLayer(\n",
       "        (attention): BERTAttention(\n",
       "          (self): BERTSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BERTSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BERTLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BERTIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BERTOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BERTLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BERTLayer(\n",
       "        (attention): BERTAttention(\n",
       "          (self): BERTSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BERTSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BERTLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BERTIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BERTOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BERTLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BERTLayer(\n",
       "        (attention): BERTAttention(\n",
       "          (self): BERTSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BERTSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BERTLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BERTIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BERTOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BERTLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BERTLayer(\n",
       "        (attention): BERTAttention(\n",
       "          (self): BERTSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BERTSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BERTLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BERTIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BERTOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BERTLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BERTPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_bert.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JHXH1X2W7SxY",
    "outputId": "658f85ba-ceac-4568-98e6-00c1b2b82b97"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BERTEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): BERTLayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BERTEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BERTLayer(\n",
       "        (attention): BERTAttention(\n",
       "          (self): BERTSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BERTSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BERTLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BERTIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BERTOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BERTLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BERTLayer(\n",
       "        (attention): BERTAttention(\n",
       "          (self): BERTSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BERTSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BERTLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BERTIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BERTOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BERTLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BERTLayer(\n",
       "        (attention): BERTAttention(\n",
       "          (self): BERTSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BERTSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BERTLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BERTIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BERTOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BERTLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BERTLayer(\n",
       "        (attention): BERTAttention(\n",
       "          (self): BERTSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BERTSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BERTLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BERTIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BERTOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BERTLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BERTLayer(\n",
       "        (attention): BERTAttention(\n",
       "          (self): BERTSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BERTSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BERTLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BERTIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BERTOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BERTLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BERTLayer(\n",
       "        (attention): BERTAttention(\n",
       "          (self): BERTSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BERTSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BERTLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BERTIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BERTOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BERTLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BERTLayer(\n",
       "        (attention): BERTAttention(\n",
       "          (self): BERTSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BERTSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BERTLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BERTIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BERTOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BERTLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BERTLayer(\n",
       "        (attention): BERTAttention(\n",
       "          (self): BERTSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BERTSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BERTLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BERTIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BERTOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BERTLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BERTLayer(\n",
       "        (attention): BERTAttention(\n",
       "          (self): BERTSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BERTSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BERTLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BERTIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BERTOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BERTLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BERTLayer(\n",
       "        (attention): BERTAttention(\n",
       "          (self): BERTSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BERTSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BERTLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BERTIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BERTOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BERTLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BERTLayer(\n",
       "        (attention): BERTAttention(\n",
       "          (self): BERTSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BERTSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BERTLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BERTIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BERTOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BERTLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BERTLayer(\n",
       "        (attention): BERTAttention(\n",
       "          (self): BERTSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BERTSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BERTLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BERTIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BERTOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BERTLayerNorm()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BERTPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_bert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3A4kiBh_bUzf"
   },
   "source": [
    "# Emedding Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "q4Zb9k35xebB"
   },
   "outputs": [],
   "source": [
    "agg_ops = ['', 'MAX', 'MIN', 'COUNT', 'SUM', 'AVG']\n",
    "cond_ops = ['=', '>', '<', 'OP'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "YjeqJoo6aWUZ"
   },
   "outputs": [],
   "source": [
    "def gen_l_hpu(i_hds):\n",
    "    \"\"\"\n",
    "    # Treat columns as if it is a batch of natural language utterance with batch-size = # of columns * # of batch_size\n",
    "    i_hds = [(17, 18), (19, 21), (22, 23), (24, 25), (26, 29), (30, 34)])\n",
    "    \"\"\"\n",
    "    l_hpu = []\n",
    "    for i_hds1 in i_hds:\n",
    "        for i_hds11 in i_hds1:\n",
    "            l_hpu.append(i_hds11[1] - i_hds11[0])\n",
    "\n",
    "    return l_hpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "S6nZ9Dw7yOD7"
   },
   "outputs": [],
   "source": [
    "def get_fields(t1s, tables):\n",
    "  nlu, nlu_t, tid, sql_i, sql_q, sql_t, tb, hs_t, hs,g_wvi_corenlp = [], [], [], [], [], [], [], [], [], []\n",
    "  for t1 in t1s:\n",
    "      \n",
    "      nlu1, nlu_t1, tid1, sql_i1, sql_q1,g_wvi_corenlp1= t1['question'],t1['question_tok'],t1['table_id'],t1['sql'],t1['query'],t1['wvi_corenlp']\n",
    "      tb1 = tables[tid1]\n",
    "      hs1 = tb1['header']\n",
    "\n",
    "      nlu.append(nlu1)\n",
    "      nlu_t.append(nlu_t1)\n",
    "      tid.append(tid1)\n",
    "      sql_i.append(sql_i1)\n",
    "      sql_q.append(sql_q1)\n",
    "      g_wvi_corenlp.append(g_wvi_corenlp1)\n",
    "\n",
    "      tb.append(tb1)\n",
    "      hs.append(hs1)\n",
    "\n",
    "  return nlu, nlu_t, sql_i, sql_q, tb, hs,g_wvi_corenlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "ANLpR_kOV8e4"
   },
   "outputs": [],
   "source": [
    "def get_g(sql_i):\n",
    "    g_sc = []\n",
    "    g_sa = []\n",
    "    g_wn = []\n",
    "    g_wc = []\n",
    "    g_wo = []\n",
    "    g_wv = []\n",
    "    for b, psql_i1 in enumerate(sql_i):\n",
    "        g_sc.append( psql_i1[\"sel\"] )\n",
    "        g_sa.append( psql_i1[\"agg\"])\n",
    "\n",
    "        \"\"\"\n",
    "        [ [wc, wo, wv],\n",
    "          [wc, wo, wv], ...\n",
    "        ]\n",
    "        \"\"\"\n",
    "\n",
    "        conds = psql_i1['conds']\n",
    "        # print(\"conds = \",conds)\n",
    "        if not psql_i1[\"agg\"] < 0:\n",
    "            g_wn.append( len( conds ) )\n",
    "            wc=[]\n",
    "            wo=[]\n",
    "            wv=[]\n",
    "            # print(wc,wo,wv)\n",
    "            # print(\"\\n CONDS = \",conds)\n",
    "            # print('\\nsingle loop ')\n",
    "            for cond in conds:\n",
    "                # print(\"\\n\",cond)\n",
    "                # print(\"split : \",cond[0])\n",
    "                # print(\"\\nWc l : \",wc)\n",
    "                wc.append( cond[0] )\n",
    "                wo.append( cond[1] )\n",
    "                wv.append( cond[2] )\n",
    "            # print(\"Ws = \",wc,wo,wv)  \n",
    "            g_wc.append(wc)\n",
    "            g_wo.append(wo)\n",
    "            g_wv.append(wv)\n",
    "            # print(\"\\nGWO : \",g_wo)  \n",
    "        else:\n",
    "            raise EnvironmentError\n",
    "    return g_sc, g_sa, g_wn, g_wc, g_wo, g_wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "wV7Zz4hBb5TQ"
   },
   "outputs": [],
   "source": [
    "def generate_inputs(nlu1_tok, hds1):\n",
    "    tokens = []\n",
    "    segment_ids = []\n",
    "    # print('grn _ inps = ')\n",
    "    # print(nlu1_tok)\n",
    "    # print(hds1)\n",
    "    tokens.append(\"[CLS]\")\n",
    "    i_st_nlu = len(tokens)  # to use it later\n",
    "\n",
    "    segment_ids.append(0)\n",
    "    for token in nlu1_tok:\n",
    "        tokens.append(token)\n",
    "        segment_ids.append(0)\n",
    "    i_ed_nlu = len(tokens)\n",
    "    tokens.append(\"[SEP]\")\n",
    "    segment_ids.append(0)\n",
    "\n",
    "    i_hds = []\n",
    "    # for doc\n",
    "    for i, hds11 in enumerate(hds1):\n",
    "        i_st_hd = len(tokens)\n",
    "        sub_tok = tokenizer(hds11)\n",
    "        tokens += sub_tok\n",
    "        i_ed_hd = len(tokens)\n",
    "        i_hds.append((i_st_hd, i_ed_hd))\n",
    "        segment_ids += [1] * len(sub_tok)\n",
    "        if i < len(hds1)-1:\n",
    "            tokens.append(\"[SEP]\")\n",
    "            segment_ids.append(0)\n",
    "        elif i == len(hds1)-1:\n",
    "            tokens.append(\"[SEP]\")\n",
    "            segment_ids.append(1)\n",
    "        else:\n",
    "            raise EnvironmentError\n",
    "\n",
    "    i_nlu = (i_st_nlu, i_ed_nlu)\n",
    "\n",
    "    return tokens, segment_ids, i_nlu, i_hds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "zjcpM4Ji1XQE"
   },
   "outputs": [],
   "source": [
    "def get_bert_output(model_bert, nlu_t, hds, max_seq_length):\n",
    "\n",
    "    l_n = []\n",
    "    l_hs = []  # The length of columns for each batch\n",
    "\n",
    "    input_ids = []\n",
    "    tokens = []\n",
    "    segment_ids = []\n",
    "    input_mask = []\n",
    "\n",
    "    i_nlu = []  # index to retreive the position of contextual vector later.\n",
    "    i_hds = []\n",
    "\n",
    "    doc_tokens = []\n",
    "    nlu_tt = []\n",
    "\n",
    "    t_to_tt_idx = []\n",
    "    tt_to_t_idx = []\n",
    "    # print('nlu_t = ',nlu_t,type(nlu_t))\n",
    "    count=0\n",
    "    for b, nlu_t1 in enumerate(nlu_t):\n",
    "\n",
    "        hds1 = hds[b]\n",
    "        l_hs.append(len(hds1))\n",
    "\n",
    "\n",
    "        # 1. 2nd tokenization using WordPiece\n",
    "        tt_to_t_idx1 = []  # number indicates where sub-token belongs to in 1st-level-tokens (here, CoreNLP).\n",
    "        t_to_tt_idx1 = []  # orig_to_tok_idx[i] = start index of i-th-1st-level-token in all_tokens.\n",
    "        nlu_tt1 = []  # all_doc_tokens[ orig_to_tok_idx[i] ] returns first sub-token segement of i-th-1st-level-token\n",
    "        for (i, token) in enumerate(nlu_t1):\n",
    "            t_to_tt_idx1.append(\n",
    "                len(nlu_tt1))  # all_doc_tokens[ indicate the start position of original 'white-space' tokens.\n",
    "            sub_tokens = tokenizer(token)\n",
    "            for sub_token in sub_tokens:\n",
    "                tt_to_t_idx1.append(i)\n",
    "                nlu_tt1.append(sub_token)  # all_doc_tokens are further tokenized using WordPiece tokenizer\n",
    "        nlu_tt.append(nlu_tt1)\n",
    "        tt_to_t_idx.append(tt_to_t_idx1)\n",
    "        t_to_tt_idx.append(t_to_tt_idx1)\n",
    "\n",
    "        # print(\"\\n------------------------------------\\n\")\n",
    "        # print(nlu_tt1)\n",
    "        # print(len(nlu_tt1))\n",
    "        # print(\"\\n----------------------\\n\")\n",
    "\n",
    "        l_n.append(len(nlu_tt1))\n",
    "        #         hds1_all_tok = tokenize_hds1(tokenizer, hds1)\n",
    "\n",
    "\n",
    "\n",
    "        # [CLS] nlu [SEP] col1 [SEP] col2 [SEP] ...col-n [SEP]\n",
    "        # 2. Generate BERT inputs & indices.\n",
    "        tokens1, segment_ids1, i_nlu1, i_hds1 = generate_inputs(nlu_tt1, hds1)\n",
    "        \n",
    "        # print(tokens1)\n",
    "        \n",
    "        input_ids1 = convert_tokens_to_ids(tokens1)\n",
    "\n",
    "        # Input masks\n",
    "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "        # tokens are attended to.\n",
    "        input_mask1 = [1] * len(input_ids1)\n",
    "\n",
    "        # 3. Zero-pad up to the sequence length.\n",
    "        while len(input_ids1) < max_seq_length:\n",
    "            input_ids1.append(0)\n",
    "            input_mask1.append(0)\n",
    "            segment_ids1.append(0)\n",
    "        if len(input_ids1) != max_seq_length or len(input_mask1) != max_seq_length or len(segment_ids1) != max_seq_length:\n",
    "            print(\"ids1 = \",len(input_ids1),max_seq_length)\n",
    "            print(\"mask1 = \",len(input_mask1),max_seq_length)\n",
    "            print(\"seg_ids1 = \",len(segment_ids1),max_seq_length)\n",
    "        assert len(input_ids1) == max_seq_length\n",
    "        assert len(input_mask1) == max_seq_length\n",
    "        assert len(segment_ids1) == max_seq_length\n",
    "\n",
    "        input_ids.append(input_ids1)\n",
    "        tokens.append(tokens1)\n",
    "        segment_ids.append(segment_ids1)\n",
    "        input_mask.append(input_mask1)\n",
    "\n",
    "        i_nlu.append(i_nlu1)\n",
    "        i_hds.append(i_hds1)\n",
    "\n",
    "    # print('Bert inputs = ')\n",
    "    # print(input_ids)\n",
    "    # print(input_mask)\n",
    "    # print(segment_ids)\n",
    "\n",
    "    # Convert to tensor\n",
    "    all_input_ids = torch.tensor(input_ids, dtype=torch.long).to(device)\n",
    "    all_input_mask = torch.tensor(input_mask, dtype=torch.long).to(device)\n",
    "    all_segment_ids = torch.tensor(segment_ids, dtype=torch.long).to(device)\n",
    "\n",
    "    # 4. Generate BERT output.\n",
    "    all_encoder_layer, pooled_output = model_bert(all_input_ids, all_segment_ids, all_input_mask)\n",
    "    #make_dot(pooled_output)\n",
    "\n",
    "    # 5. generate l_hpu from i_hds\n",
    "    l_hpu = gen_l_hpu(i_hds)\n",
    "\n",
    "    return all_encoder_layer, pooled_output, tokens, i_nlu, i_hds, l_n, l_hpu, l_hs, nlu_tt, t_to_tt_idx, tt_to_t_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "-Hr6lxhm7RZE"
   },
   "outputs": [],
   "source": [
    "def get_wemb_bert(hidden_size, num_hidden_layers, model_bert, nlu_t, hds, max_seq_length, num_out_layers_n=num_target_layers, num_out_layers_h=num_target_layers):\n",
    "\n",
    "    # get contextual output of all tokens from bert\n",
    "    all_encoder_layer, pooled_output, tokens, i_nlu, i_hds,\\\n",
    "    l_n, l_hpu, l_hs, \\\n",
    "    nlu_tt, t_to_tt_idx, tt_to_t_idx = get_bert_output(model_bert, nlu_t, hds, max_seq_length)\n",
    " \n",
    "\n",
    "    # print(\"\\nall_encoder = \",all_encoder_layer)\n",
    "    # print(len(all_encoder_layer))\n",
    "    # get the wemb\n",
    "    wemb_n = get_wemb_n(i_nlu, l_n, hidden_size, num_hidden_layers, all_encoder_layer,\n",
    "                        num_out_layers_n)\n",
    "\n",
    "    wemb_h = get_wemb_h(i_hds, l_hpu, l_hs, hidden_size, num_hidden_layers, all_encoder_layer,\n",
    "                        num_out_layers_h)\n",
    "\n",
    "    return wemb_n, wemb_h, l_n, l_hpu, l_hs,nlu_tt, t_to_tt_idx, tt_to_t_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "dwsEECbo2bHp"
   },
   "outputs": [],
   "source": [
    "def get_wemb_n(i_nlu, l_n, hS, num_hidden_layers, all_encoder_layer, num_out_layers_n):\n",
    "    \"\"\"\n",
    "    Get the representation of each tokens.\n",
    "    \"\"\"\n",
    "    bS = len(l_n)\n",
    "    l_n_max = max(l_n)\n",
    "    wemb_n = torch.zeros([bS, l_n_max, hS * num_out_layers_n]).to(device)\n",
    "    for b in range(bS):\n",
    "        # [B, max_len, dim]\n",
    "        # Fill zero for non-exist part.\n",
    "        l_n1 = l_n[b]\n",
    "        i_nlu1 = i_nlu[b]\n",
    "        for i_noln in range(num_out_layers_n):\n",
    "            i_layer = num_hidden_layers - 1 - i_noln\n",
    "            st = i_noln * hS\n",
    "            ed = (i_noln + 1) * hS\n",
    "            wemb_n[b, 0:(i_nlu1[1] - i_nlu1[0]), st:ed] = all_encoder_layer[i_layer][b, i_nlu1[0]:i_nlu1[1], :]\n",
    "    return wemb_n\n",
    " \n",
    "\n",
    "\n",
    "def get_wemb_h(i_hds, l_hpu, l_hs, hS, num_hidden_layers, all_encoder_layer, num_out_layers_h):\n",
    "    \n",
    "    bS = len(l_hs)\n",
    "    l_hpu_max = max(l_hpu)\n",
    "    num_of_all_hds = sum(l_hs)\n",
    "    wemb_h = torch.zeros([num_of_all_hds, l_hpu_max, hS * num_out_layers_h]).to(device)\n",
    "    b_pu = -1\n",
    "    for b, i_hds1 in enumerate(i_hds):\n",
    "        for b1, i_hds11 in enumerate(i_hds1):\n",
    "            b_pu += 1\n",
    "            for i_nolh in range(num_out_layers_h):\n",
    "                i_layer = num_hidden_layers - 1 - i_nolh\n",
    "                st = i_nolh * hS\n",
    "                ed = (i_nolh + 1) * hS\n",
    "                wemb_h[b_pu, 0:(i_hds11[1] - i_hds11[0]), st:ed] \\\n",
    "                    = all_encoder_layer[i_layer][b, i_hds11[0]:i_hds11[1],:]\n",
    "\n",
    "\n",
    "    return wemb_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "HYcYScv7khVL"
   },
   "outputs": [],
   "source": [
    "def get_g_wvi_bert_from_g_wvi_corenlp(wh_to_wp_index, g_wvi_corenlp):\n",
    "\n",
    "    g_wvi = []\n",
    "    for b, g_wvi_corenlp1 in enumerate(g_wvi_corenlp):\n",
    "        wh_to_wp_index1 = wh_to_wp_index[b]\n",
    "        g_wvi1 = []\n",
    "        for i_wn, g_wvi_corenlp11 in enumerate(g_wvi_corenlp1):\n",
    "\n",
    "            st_idx, ed_idx = g_wvi_corenlp11\n",
    "\n",
    "            st_wp_idx = wh_to_wp_index1[st_idx]\n",
    "            ed_wp_idx = wh_to_wp_index1[ed_idx]\n",
    "\n",
    "            g_wvi11 = [st_wp_idx, ed_wp_idx]\n",
    "            g_wvi1.append(g_wvi11)\n",
    "\n",
    "        g_wvi.append(g_wvi1)\n",
    "\n",
    "    return g_wvi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "q7iu_ggKNNg0"
   },
   "outputs": [],
   "source": [
    "def encode(lstm, wemb_l, l, return_hidden=False, hc0=None, last_only=False):\n",
    "    \"\"\" [batch_size, max token length, dim_emb]\n",
    "    \"\"\"\n",
    "    bS, mL, eS = wemb_l.shape\n",
    "\n",
    "\n",
    "    # sort before packking\n",
    "    l = np.array(l)\n",
    "    perm_idx = np.argsort(-l)\n",
    "    perm_idx_inv = np.zeros(len(perm_idx), dtype=np.int32)\n",
    "    for i, p in enumerate(perm_idx):\n",
    "        perm_idx_inv[int(p)] = i\n",
    "\n",
    "    # pack sequence\n",
    "\n",
    "    packed_wemb_l = nn.utils.rnn.pack_padded_sequence(wemb_l[perm_idx, :, :],\n",
    "                                                      l[perm_idx],\n",
    "                                                      batch_first=True)\n",
    "\n",
    "    # Time to encode\n",
    "    if hc0 is not None:\n",
    "        hc0 = (hc0[0][:, perm_idx], hc0[1][:, perm_idx])\n",
    "\n",
    "    # ipdb.set_trace()\n",
    "    packed_wemb_l = packed_wemb_l.float() # I don't know why..\n",
    "    packed_wenc, hc_out = lstm(packed_wemb_l, hc0)\n",
    "    hout, cout = hc_out\n",
    "\n",
    "    # unpack\n",
    "    wenc, _l = nn.utils.rnn.pad_packed_sequence(packed_wenc, batch_first=True)\n",
    "\n",
    "    if last_only:\n",
    "        # Take only final outputs for each columns.\n",
    "        wenc = wenc[tuple(range(bS)), l[perm_idx] - 1]  # [batch_size, dim_emb]\n",
    "        wenc.unsqueeze_(1)  # [batch_size, 1, dim_emb]\n",
    "\n",
    "    wenc = wenc[perm_idx_inv]\n",
    "\n",
    "\n",
    "\n",
    "    if return_hidden:\n",
    "        # hout.shape = [number_of_directoin * num_of_layer, seq_len(=batch size), dim * number_of_direction ] w/ batch_first.. w/o batch_first? I need to see.\n",
    "        hout = hout[:, perm_idx_inv].to(device)\n",
    "        cout = cout[:, perm_idx_inv].to(device)  # Is this correct operation?\n",
    "\n",
    "        return wenc, hout, cout\n",
    "    else:\n",
    "        return wenc\n",
    "\n",
    "\n",
    "def encode_hpu(lstm, wemb_hpu, l_hpu, l_hs):\n",
    "    wenc_hpu, hout, cout = encode( lstm,\n",
    "                                   wemb_hpu,\n",
    "                                   l_hpu,\n",
    "                                   return_hidden=True,\n",
    "                                   hc0=None,\n",
    "                                   last_only=True )\n",
    "\n",
    "    wenc_hpu = wenc_hpu.squeeze(1)\n",
    "    bS_hpu, mL_hpu, eS = wemb_hpu.shape\n",
    "    hS = wenc_hpu.size(-1)\n",
    "\n",
    "    wenc_hs = wenc_hpu.new_zeros(len(l_hs), max(l_hs), hS)\n",
    "    wenc_hs = wenc_hs.to(device)\n",
    "\n",
    "    # Re-pack according to batch.\n",
    "    # ret = [B_NLq, max_len_headers_all, dim_lstm]\n",
    "    st = 0\n",
    "    for i, l_hs1 in enumerate(l_hs):\n",
    "        wenc_hs[i, :l_hs1] = wenc_hpu[st:(st + l_hs1)]\n",
    "        st += l_hs1\n",
    "\n",
    "    return wenc_hs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "89RIwgb_tsI0"
   },
   "source": [
    "# LSTMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "9udgFynUtvpP"
   },
   "outputs": [],
   "source": [
    "class SCP(nn.Module):\n",
    "    def __init__(self, iS=300, hS=100, lS=2, dr=0.3):\n",
    "        super(SCP, self).__init__()\n",
    "        self.iS = iS\n",
    "        self.hS = hS\n",
    "        self.lS = lS\n",
    "        self.dr = dr\n",
    "\n",
    "        self.question_knowledge_dim = 5\n",
    "        self.header_knowledge_dim = 3\n",
    "        self.enc_h = nn.LSTM(input_size=iS, hidden_size=int(hS / 2),\n",
    "                             num_layers=lS, batch_first=True,\n",
    "                             dropout=dr, bidirectional=True)\n",
    "\n",
    "        self.enc_n = nn.LSTM(input_size=iS, hidden_size=int(hS / 2),\n",
    "                             num_layers=lS, batch_first=True,\n",
    "                             dropout=dr, bidirectional=True)\n",
    "\n",
    "        self.W_att = nn.Linear(hS + self.question_knowledge_dim, hS + self.header_knowledge_dim)\n",
    "        self.W_c = nn.Linear(hS + self.question_knowledge_dim, hS)\n",
    "        self.W_hs = nn.Linear(hS+self.header_knowledge_dim, hS)\n",
    "        self.sc_out = nn.Sequential(nn.Tanh(), nn.Linear(2 * hS, 1))\n",
    "\n",
    "        self.softmax_dim1 = nn.Softmax(dim=1)\n",
    "        self.softmax_dim2 = nn.Softmax(dim=2)\n",
    "\n",
    "    def forward(self, wemb_n, l_n, wemb_hpu, l_hpu, l_hs, show_p_sc=False,\n",
    "                knowledge=None,\n",
    "                knowledge_header=None):\n",
    "\n",
    "                \n",
    "        # Encode\n",
    "        # print(\"\\n--------------------------------\\n\")\n",
    "        # print(l_n)\n",
    "        mL_n = max(l_n)\n",
    "        bS = len(l_hs)\n",
    "        wenc_n = encode(self.enc_n, wemb_n, l_n,\n",
    "                        return_hidden=False,\n",
    "                        hc0=None,\n",
    "                        last_only=False)  # [b, n, dim]\n",
    "\n",
    "        \n",
    "        # # print(mL_n)\n",
    "        # print(\"\\nK = \",knowledge)\n",
    "        # max_k =0\n",
    "        # for single_knowledge in knowledge:\n",
    "        #     if len(single_knowledge)>max_k:\n",
    "        #         max_k = len(single_knowledge)\n",
    "        # print(max_k,mL_n)\n",
    "\n",
    "        # if max_k != mL_n :\n",
    "        #   count +=1\n",
    "\n",
    "        knowledge = [k + (mL_n - len(k)) * [0] for k in knowledge]\n",
    "        # knowledge = [k + (max_k - len(k)) * [0] for k in knowledge]\n",
    "        # print('\\nKNOW = ', knowledge.shape)\n",
    "        # print(\"\\n--------------------------------\\n\")\n",
    "        \n",
    "        # print(\"K = \",knowledge,\"\\n\")\n",
    "        # # print(len(knowledge),len(knowledge[0]))\n",
    "        \n",
    "        knowledge = torch.tensor(knowledge).unsqueeze(-1)\n",
    "        # print(\"Unsquus done : \",knowledge.shape)\n",
    "        # # print(\"\\n--------------------------------\\n\")\n",
    "\n",
    "        feature = torch.zeros(bS, mL_n, self.question_knowledge_dim).scatter_(dim=-1,\n",
    "                                                                              index=knowledge,\n",
    "                                                                              value=1).to(device)\n",
    "\n",
    "        # feature = torch.zeros(bS, max_k, self.question_knowledge_dim).scatter_(dim=-1,\n",
    "        #                                                                       index=knowledge,\n",
    "        #                                                                       value=1).to(device)\n",
    "\n",
    "        # print(feature.shape)\n",
    "        # print(wenc_n.shape,\"\\n\")\n",
    "        wenc_n = torch.cat([wenc_n, feature], -1)\n",
    "        wenc_hs = encode_hpu(self.enc_h, wemb_hpu, l_hpu, l_hs)  # [b, hs, dim]\n",
    "        knowledge_header = [k + (max(l_hs) - len(k)) * [0] for k in knowledge_header]\n",
    "        knowledge_header = torch.tensor(knowledge_header).unsqueeze(-1)\n",
    "        feature2 = torch.zeros(bS, max(l_hs), self.header_knowledge_dim).scatter_(dim=-1,\n",
    "                                                                                        index=knowledge_header,\n",
    "                                                                                        value=1).to(device)\n",
    "        wenc_hs = torch.cat([wenc_hs, feature2], -1)\n",
    "        bS = len(l_hs)\n",
    "        mL_n = max(l_n)\n",
    "\n",
    "        #   [bS, mL_hs, 100] * [bS, 100, mL_n] -> [bS, mL_hs, mL_n]\n",
    "        att_h = torch.bmm(wenc_hs, self.W_att(wenc_n).transpose(1, 2))\n",
    "\n",
    "        #   Penalty on blank parts\n",
    "        for b, l_n1 in enumerate(l_n):\n",
    "            if l_n1 < mL_n:\n",
    "                att_h[b, :, l_n1:] = -10000000000\n",
    "\n",
    "        p_n = self.softmax_dim2(att_h)\n",
    "\n",
    "        #   p_n [ bS, mL_hs, mL_n]  -> [ bS, mL_hs, mL_n, 1]\n",
    "        #   wenc_n [ bS, mL_n, 100] -> [ bS, 1, mL_n, 100]\n",
    "        #   -> [bS, mL_hs, mL_n, 100] -> [bS, mL_hs, 100]\n",
    "        c_n = torch.mul(p_n.unsqueeze(3), wenc_n.unsqueeze(1)).sum(dim=2)\n",
    "\n",
    "        vec = torch.cat([self.W_c(c_n), self.W_hs(wenc_hs)], dim=2)\n",
    "        s_sc = self.sc_out(vec).squeeze(2) # [bS, mL_hs, 1] -> [bS, mL_hs]\n",
    "\n",
    "\n",
    "        # Penalty\n",
    "        mL_hs = max(l_hs)\n",
    "        for b, l_hs1 in enumerate(l_hs):\n",
    "            if l_hs1 < mL_hs:\n",
    "                s_sc[b, l_hs1:] = -10000000000\n",
    "\n",
    "        return s_sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "yk8nRWvTNY6F"
   },
   "outputs": [],
   "source": [
    "class SAP(nn.Module):\n",
    "    def __init__(self, iS=300, hS=100, lS=2, dr=0.3, n_agg_ops=-1, old=False):\n",
    "        super(SAP, self).__init__()\n",
    "        self.iS = iS\n",
    "        self.hS = hS\n",
    "        self.lS = lS\n",
    "        self.dr = dr\n",
    "\n",
    "        self.question_knowledge_dim = 5\n",
    "        self.header_knowledge_dim = 3\n",
    "        self.enc_h = nn.LSTM(input_size=iS, hidden_size=int(hS / 2),\n",
    "                             num_layers=lS, batch_first=True,\n",
    "                             dropout=dr, bidirectional=True)\n",
    "\n",
    "        self.enc_n = nn.LSTM(input_size=iS, hidden_size=int(hS / 2),\n",
    "                             num_layers=lS, batch_first=True,\n",
    "                             dropout=dr, bidirectional=True)\n",
    "\n",
    "        self.W_att = nn.Linear(hS + self.question_knowledge_dim, hS + self.header_knowledge_dim)\n",
    "        self.sa_out = nn.Sequential(nn.Linear(hS + self.question_knowledge_dim, hS),\n",
    "                                    nn.Tanh(),\n",
    "                                    nn.Linear(hS, n_agg_ops))  # Fixed number of aggregation operator.\n",
    "\n",
    "        self.softmax_dim1 = nn.Softmax(dim=1)\n",
    "        self.softmax_dim2 = nn.Softmax(dim=2)\n",
    "\n",
    "        if old:\n",
    "            # for backwoard compatibility\n",
    "            self.W_c = nn.Linear(hS, hS)\n",
    "            self.W_hs = nn.Linear(hS, hS)\n",
    "\n",
    "    # wemb_hpu [batch_size*header_num, max_header_len, hidden_dim]\n",
    "    # l_hpu [batch_size*header_num]\n",
    "    # l_hs [batch_size]\n",
    "    def forward(self, wemb_n, l_n, wemb_hpu, l_hpu, l_hs, pr_sc, show_p_sa=False,\n",
    "                knowledge=None,\n",
    "                knowledge_header=None):\n",
    "        # Encode\n",
    "        mL_n = max(l_n)\n",
    "        bS = len(l_hs)\n",
    "        wenc_n = encode(self.enc_n, wemb_n, l_n,\n",
    "                        return_hidden=False,\n",
    "                        hc0=None,\n",
    "                        last_only=False)  # [b, n, dim]\n",
    "        knowledge = [k + (mL_n - len(k)) * [0] for k in knowledge]\n",
    "        knowledge = torch.tensor(knowledge).unsqueeze(-1)\n",
    "\n",
    "        feature = torch.zeros(bS, mL_n, self.question_knowledge_dim).scatter_(dim=-1,\n",
    "                                                                              index=knowledge,\n",
    "                                                                              value=1).to(device)\n",
    "        wenc_n = torch.cat([wenc_n, feature], -1)\n",
    "\n",
    "        wenc_hs = encode_hpu(self.enc_h, wemb_hpu, l_hpu, l_hs)  # [b, hs, dim]\n",
    "        knowledge_header = [k + (max(l_hs) - len(k)) * [0] for k in knowledge_header]\n",
    "        knowledge_header = torch.tensor(knowledge_header).unsqueeze(-1)\n",
    "        feature2 = torch.zeros(bS, max(l_hs), self.header_knowledge_dim).scatter_(dim=-1,\n",
    "                                                                                        index=knowledge_header,\n",
    "                                                                                        value=1).to(device)\n",
    "        wenc_hs = torch.cat([wenc_hs, feature2], -1)\n",
    "        bS = len(l_hs)\n",
    "        mL_n = max(l_n)\n",
    "\n",
    "        wenc_hs_ob = wenc_hs[list(range(bS)), pr_sc]  # list, so one sample for each batch.\n",
    "\n",
    "        # [bS, mL_n, 100] * [bS, 100, 1] -> [bS, mL_n]\n",
    "        att = torch.bmm(self.W_att(wenc_n), wenc_hs_ob.unsqueeze(2)).squeeze(2)\n",
    "\n",
    "        #   Penalty on blank parts\n",
    "        for b, l_n1 in enumerate(l_n):\n",
    "            if l_n1 < mL_n:\n",
    "                att[b, l_n1:] = -10000000000\n",
    "        # [bS, mL_n]\n",
    "        p = self.softmax_dim1(att)\n",
    "\n",
    "            \n",
    "        #    [bS, mL_n, 100] * ( [bS, mL_n, 1] -> [bS, mL_n, 100])\n",
    "        #       -> [bS, mL_n, 100] -> [bS, 100]\n",
    "        c_n = torch.mul(wenc_n, p.unsqueeze(2).expand_as(wenc_n)).sum(dim=1)\n",
    "        s_sa = self.sa_out(c_n)\n",
    "\n",
    "        return s_sa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "LMeSPt2YNsYR"
   },
   "outputs": [],
   "source": [
    "class WNP(nn.Module):\n",
    "    def __init__(self, iS=300, hS=100, lS=2, dr=0.3, ):\n",
    "        super(WNP, self).__init__()\n",
    "        self.iS = iS\n",
    "        self.hS = hS\n",
    "        self.lS = lS\n",
    "        self.dr = dr\n",
    "\n",
    "        self.mL_w = 4  # max where condition number\n",
    "        self.question_knowledge_dim = 5\n",
    "        self.header_knowledge_dim = 3\n",
    "        self.enc_h = nn.LSTM(input_size=iS, hidden_size=int(hS / 2),\n",
    "                             num_layers=lS, batch_first=True,\n",
    "                             dropout=dr, bidirectional=True)\n",
    "\n",
    "        self.enc_n = nn.LSTM(input_size=iS, hidden_size=int(hS / 2),\n",
    "                             num_layers=lS, batch_first=True,\n",
    "                             dropout=dr, bidirectional=True)\n",
    "\n",
    "        self.W_att_h = nn.Linear(hS + self.header_knowledge_dim, 1)\n",
    "        self.W_hidden = nn.Linear(hS + self.header_knowledge_dim, lS * hS)\n",
    "        self.W_cell = nn.Linear(hS + self.header_knowledge_dim, lS * hS)\n",
    "\n",
    "        self.W_att_n = nn.Linear(hS + self.question_knowledge_dim, 1)\n",
    "        self.wn_out = nn.Sequential(nn.Linear(hS + self.question_knowledge_dim, hS),\n",
    "                                    nn.Tanh(),\n",
    "                                    nn.Linear(hS, self.mL_w + 1))  # max number (4 + 1)\n",
    "\n",
    "        self.softmax_dim1 = nn.Softmax(dim=1)\n",
    "        self.softmax_dim2 = nn.Softmax(dim=2)\n",
    "\n",
    "    def forward(self, wemb_n, l_n, wemb_hpu, l_hpu, l_hs, show_p_wn=False,\n",
    "                    knowledge = None,\n",
    "                    knowledge_header = None):\n",
    "        # Encode\n",
    "        mL_n = max(l_n)\n",
    "        bS = len(l_hs)\n",
    "        wenc_hs = encode_hpu(self.enc_h, wemb_hpu, l_hpu, l_hs)  # [b, mL_hs, dim]\n",
    "        knowledge_header = [k + (max(l_hs) - len(k)) * [0] for k in knowledge_header]\n",
    "        knowledge_header = torch.tensor(knowledge_header).unsqueeze(-1)\n",
    "        feature2 = torch.zeros(bS, max(l_hs), self.header_knowledge_dim).scatter_(dim=-1,\n",
    "                                                                                        index=knowledge_header,\n",
    "                                                                                        value=1).to(device)\n",
    "        wenc_hs = torch.cat([wenc_hs, feature2], -1)\n",
    "\n",
    "        bS = len(l_hs)\n",
    "        mL_n = max(l_n)\n",
    "        mL_hs = max(l_hs)\n",
    "        # mL_h = max(l_hpu)\n",
    "\n",
    "        #   (self-attention?) column Embedding?\n",
    "        #   [B, mL_hs, 100] -> [B, mL_hs, 1] -> [B, mL_hs]\n",
    "        att_h = self.W_att_h(wenc_hs).squeeze(2)\n",
    "\n",
    "        #   Penalty\n",
    "        for b, l_hs1 in enumerate(l_hs):\n",
    "            if l_hs1 < mL_hs:\n",
    "                att_h[b, l_hs1:] = -10000000000\n",
    "        p_h = self.softmax_dim1(att_h)\n",
    "\n",
    "\n",
    "        #   [B, mL_hs, 100] * [ B, mL_hs, 1] -> [B, mL_hs, 100] -> [B, 100]\n",
    "        c_hs = torch.mul(wenc_hs, p_h.unsqueeze(2)).sum(1)\n",
    "\n",
    "        #   [B, 100] --> [B, 2*100] Enlarge because there are two layers.\n",
    "        hidden = self.W_hidden(c_hs)  # [B, 4, 200/2]\n",
    "        hidden = hidden.view(bS, self.lS * 2, int(\n",
    "            self.hS / 2))  # [4, B, 100/2] # number_of_layer_layer * (bi-direction) # lstm input convention.\n",
    "        hidden = hidden.transpose(0, 1).contiguous()\n",
    "\n",
    "        cell = self.W_cell(c_hs)  # [B, 4, 100/2]\n",
    "        cell = cell.view(bS, self.lS * 2, int(self.hS / 2))  # [4, B, 100/2]\n",
    "        cell = cell.transpose(0, 1).contiguous()\n",
    "\n",
    "        wenc_n = encode(self.enc_n, wemb_n, l_n,\n",
    "                        return_hidden=False,\n",
    "                        hc0=(hidden, cell),\n",
    "                        last_only=False)  # [b, n, dim]\n",
    "\n",
    "        knowledge = [k + (mL_n - len(k)) * [0] for k in knowledge]\n",
    "        knowledge = torch.tensor(knowledge).unsqueeze(-1)\n",
    "\n",
    "        feature = torch.zeros(bS, mL_n, self.question_knowledge_dim).scatter_(dim=-1,\n",
    "                                                                              index=knowledge,\n",
    "                                                                              value=1).to(device)\n",
    "        wenc_n = torch.cat([wenc_n, feature], -1)\n",
    "\n",
    "        att_n = self.W_att_n(wenc_n).squeeze(2)  # [B, max_len, 100] -> [B, max_len, 1] -> [B, max_len]\n",
    "\n",
    "        #    Penalty\n",
    "        for b, l_n1 in enumerate(l_n):\n",
    "            if l_n1 < mL_n:\n",
    "                att_n[b, l_n1:] = -10000000000\n",
    "        p_n = self.softmax_dim1(att_n)\n",
    "\n",
    "        \n",
    "            # input('Type Enter to continue.')\n",
    "\n",
    "        #    [B, mL_n, 100] *([B, mL_n] -> [B, mL_n, 1] -> [B, mL_n, 100] ) -> [B, 100]\n",
    "        c_n = torch.mul(wenc_n, p_n.unsqueeze(2).expand_as(wenc_n)).sum(dim=1)\n",
    "        s_wn = self.wn_out(c_n)\n",
    "\n",
    "        return s_wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "Puq-G8NyN3gX"
   },
   "outputs": [],
   "source": [
    "class WCP(nn.Module):\n",
    "    def __init__(self, iS=300, hS=100, lS=2, dr=0.3):\n",
    "        super(WCP, self).__init__()\n",
    "        self.iS = iS\n",
    "        self.hS = hS\n",
    "        self.lS = lS\n",
    "        self.dr = dr\n",
    "        self.question_knowledge_dim = 5\n",
    "        self.header_knowledge_dim = 3\n",
    "        self.enc_h = nn.LSTM(input_size=iS, hidden_size=int(hS / 2),\n",
    "                             num_layers=lS, batch_first=True,\n",
    "                             dropout=dr, bidirectional=True)\n",
    "\n",
    "        self.enc_n = nn.LSTM(input_size=iS, hidden_size=int(hS / 2),\n",
    "                             num_layers=lS, batch_first=True,\n",
    "                             dropout=dr, bidirectional=True)\n",
    "\n",
    "        self.W_att = nn.Linear(hS + self.question_knowledge_dim, hS + self.header_knowledge_dim)\n",
    "        self.W_c = nn.Linear(hS + self.question_knowledge_dim, hS)\n",
    "        self.W_hs = nn.Linear(hS + self.header_knowledge_dim, hS)\n",
    "        self.W_out = nn.Sequential(\n",
    "            nn.Tanh(), nn.Linear(2 * hS, 1)\n",
    "        )\n",
    "\n",
    "        self.softmax_dim1 = nn.Softmax(dim=1)\n",
    "        self.softmax_dim2 = nn.Softmax(dim=2)\n",
    "\n",
    "    def forward(self, wemb_n, l_n, wemb_hpu, l_hpu, l_hs, show_p_wc, penalty=True, predict_select_column=None,\n",
    "                knowledge=None,\n",
    "                knowledge_header=None):\n",
    "        # Encode\n",
    "        mL_n = max(l_n)\n",
    "        bS = len(l_hs)\n",
    "        wenc_n = encode(self.enc_n, wemb_n, l_n,\n",
    "                        return_hidden=False,\n",
    "                        hc0=None,\n",
    "                        last_only=False)  # [b, n, dim]\n",
    "        knowledge = [k + (mL_n - len(k)) * [0] for k in knowledge]\n",
    "        knowledge = torch.tensor(knowledge).unsqueeze(-1)\n",
    "\n",
    "        feature = torch.zeros(bS, mL_n, self.question_knowledge_dim).scatter_(dim=-1,\n",
    "                                                                              index=knowledge,\n",
    "                                                                              value=1).to(device)\n",
    "        wenc_n = torch.cat([wenc_n, feature], -1)\n",
    "\n",
    "        wenc_hs = encode_hpu(self.enc_h, wemb_hpu, l_hpu, l_hs)  # [b, hs, dim]\n",
    "        knowledge_header = [k + (max(l_hs) - len(k)) * [0] for k in knowledge_header]\n",
    "        knowledge_header = torch.tensor(knowledge_header).unsqueeze(-1)\n",
    "        feature2 = torch.zeros(bS, max(l_hs), self.header_knowledge_dim).scatter_(dim=-1,\n",
    "                                                                                        index=knowledge_header,\n",
    "                                                                                        value=1).to(device)\n",
    "        wenc_hs = torch.cat([wenc_hs, feature2], -1)\n",
    "        # attention\n",
    "        # wenc = [bS, mL, hS]\n",
    "        # att = [bS, mL_hs, mL_n]\n",
    "        # att[b, i_h, j_n] = p(j_n| i_h)\n",
    "        att = torch.bmm(wenc_hs, self.W_att(wenc_n).transpose(1, 2))\n",
    "\n",
    "        # penalty to blank part.\n",
    "        mL_n = max(l_n)\n",
    "        for b_n, l_n1 in enumerate(l_n):\n",
    "            if l_n1 < mL_n:\n",
    "                att[b_n, :, l_n1:] = -10000000000\n",
    "\n",
    "        # for b, c in enumerate(predict_select_column):\n",
    "        #      att[b, c, :] = -10000000000\n",
    "\n",
    "        # make p(j_n | i_h)\n",
    "        p = self.softmax_dim2(att)\n",
    "\n",
    "        \n",
    "        # max nlu context vectors\n",
    "        # [bS, mL_hs, mL_n]*[bS, mL_hs, mL_n]\n",
    "        wenc_n = wenc_n.unsqueeze(1)  # [ b, n, dim] -> [b, 1, n, dim]\n",
    "        p = p.unsqueeze(3)  # [b, hs, n] -> [b, hs, n, 1]\n",
    "        c_n = torch.mul(wenc_n, p).sum(2)  # -> [b, hs, dim], c_n for each header.\n",
    "\n",
    "        # bS = len(l_hs)\n",
    "        # index = torch.tensor(predict_select_column).unsqueeze(-1)\n",
    "        # feature = torch.zeros(bS, max(l_hs)).scatter_(dim=-1,\n",
    "        #                                                  index=index,\n",
    "        #                                                  value=1).to(device)\n",
    "        # c_n = torch.cat([c_n, feature.unsqueeze(-1)],dim=-1)\n",
    "\n",
    "        y = torch.cat([self.W_c(c_n), self.W_hs(wenc_hs)], dim=2)  # [b, hs, 2*dim]\n",
    "        score = self.W_out(y).squeeze(2)  # [b, hs]\n",
    "\n",
    "        if penalty:\n",
    "            for b, l_hs1 in enumerate(l_hs):\n",
    "                score[b, l_hs1:] = -1e+10\n",
    "\n",
    "            # for b, c in enumerate(predict_select_column):\n",
    "            #     score[b, c] = -1e+10\n",
    "\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "6bu8wmFZOC8T"
   },
   "outputs": [],
   "source": [
    "class WOP(nn.Module):\n",
    "    def __init__(self, iS=300, hS=100, lS=2, dr=0.3, n_cond_ops=3):\n",
    "        super(WOP, self).__init__()\n",
    "        self.iS = iS\n",
    "        self.hS = hS\n",
    "        self.lS = lS\n",
    "        self.dr = dr\n",
    "        self.question_knowledge_dim = 0\n",
    "        self.header_knowledge_dim = 0\n",
    "        self.mL_w = 4 # max where condition number\n",
    "\n",
    "        self.enc_h = nn.LSTM(input_size=iS, hidden_size=int(hS / 2),\n",
    "                             num_layers=lS, batch_first=True,\n",
    "                             dropout=dr, bidirectional=True)\n",
    "\n",
    "        self.enc_n = nn.LSTM(input_size=iS, hidden_size=int(hS / 2),\n",
    "                             num_layers=lS, batch_first=True,\n",
    "                             dropout=dr, bidirectional=True)\n",
    "\n",
    "        self.W_att = nn.Linear(hS + self.question_knowledge_dim, hS + self.header_knowledge_dim)\n",
    "        self.W_c = nn.Linear(hS + self.question_knowledge_dim, hS)\n",
    "        self.W_hs = nn.Linear(hS + self.header_knowledge_dim, hS)\n",
    "        self.wo_out = nn.Sequential(\n",
    "            nn.Linear(2*hS, hS),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hS, n_cond_ops)\n",
    "        )\n",
    "\n",
    "        self.softmax_dim1 = nn.Softmax(dim=1)\n",
    "        self.softmax_dim2 = nn.Softmax(dim=2)\n",
    "\n",
    "    def forward(self, wemb_n, l_n, wemb_hpu, l_hpu, l_hs, wn, wc, wenc_n=None, show_p_wo=False,\n",
    "                knowledge = None,\n",
    "                knowledge_header = None):\n",
    "        # Encode\n",
    "        mL_n = max(l_n)\n",
    "        bS = len(l_hs)\n",
    "        if not wenc_n:\n",
    "            wenc_n = encode(self.enc_n, wemb_n, l_n,\n",
    "                            return_hidden=False,\n",
    "                            hc0=None,\n",
    "                            last_only=False)  # [b, n, dim]\n",
    "            if self.question_knowledge_dim!=0:\n",
    "                knowledge = [k + (mL_n - len(k)) * [0] for k in knowledge]\n",
    "                knowledge = torch.tensor(knowledge).unsqueeze(-1)\n",
    "\n",
    "                feature = torch.zeros(bS, mL_n, self.question_knowledge_dim).scatter_(dim=-1,\n",
    "                                                                                      index=knowledge,\n",
    "                                                                                      value=1).to(device)\n",
    "                wenc_n = torch.cat([wenc_n, feature], -1)\n",
    "\n",
    "        wenc_hs = encode_hpu(self.enc_h, wemb_hpu, l_hpu, l_hs)  # [b, hs, dim]\n",
    "        if self.header_knowledge_dim != 0:\n",
    "            knowledge_header = [k + (max(l_hs) - len(k)) * [0] for k in knowledge_header]\n",
    "            knowledge_header = torch.tensor(knowledge_header).unsqueeze(-1)\n",
    "            feature2 = torch.zeros(bS, max(l_hs), self.header_knowledge_dim).scatter_(dim=-1,\n",
    "                                                                                            index=knowledge_header,\n",
    "                                                                                            value=1).to(device)\n",
    "            wenc_hs = torch.cat([wenc_hs, feature2], -1)\n",
    "\n",
    "        bS = len(l_hs)\n",
    "        # wn\n",
    "\n",
    "        # print(\"\\nWHS\",wenc_hs,\"\\n\")\n",
    "        \n",
    "        wenc_hs_ob = [] # observed hs\n",
    "        for b in range(bS):\n",
    "            # [[...], [...]]\n",
    "            # Pad list to maximum number of selections\n",
    "            \n",
    "            # print(\"\\nWC_all = \",wc,\"\\n\")\n",
    "            # print(\"WC[B]\",wc[b])\n",
    "            real = [wenc_hs[b, col] for col in wc[b]]\n",
    "            # real=[]\n",
    "            # for col in wc[b]:\n",
    "            #   # print(\"\\n Loop check :\",b,col)\n",
    "            #   print(wenc_hs[b,col])\n",
    "            #   real.append(wenc_hs[b,col])\n",
    "            # print(\"\\nREAL = \",real)\n",
    "            pad = (self.mL_w - wn[b]) * [wenc_hs[b, 0]] # this padding could be wrong. Test with zero padding later.\n",
    "            wenc_hs_ob1 = torch.stack(real + pad) # It is not used in the loss function.\n",
    "            wenc_hs_ob.append(wenc_hs_ob1)\n",
    "\n",
    "        # list to [B, 4, dim] tensor.\n",
    "        wenc_hs_ob = torch.stack(wenc_hs_ob) # list to tensor.\n",
    "        wenc_hs_ob = wenc_hs_ob.to(device)\n",
    "\n",
    "        # [B, 1, mL_n, dim] * [B, 4, dim, 1]\n",
    "        #  -> [B, 4, mL_n, 1] -> [B, 4, mL_n]\n",
    "        # multiplication bewteen NLq-tokens and  selected column\n",
    "        att = torch.matmul(self.W_att(wenc_n).unsqueeze(1),\n",
    "                              wenc_hs_ob.unsqueeze(3)\n",
    "                              ).squeeze(3)\n",
    "\n",
    "        # Penalty for blank part.\n",
    "        mL_n = max(l_n)\n",
    "        for b, l_n1 in enumerate(l_n):\n",
    "            if l_n1 < mL_n:\n",
    "                att[b, :, l_n1:] = -10000000000\n",
    "\n",
    "        p = self.softmax_dim2(att)  # p( n| selected_col )\n",
    "        \n",
    "\n",
    "        # [B, 1, mL_n, dim] * [B, 4, mL_n, 1]\n",
    "        #  --> [B, 4, mL_n, dim]\n",
    "        #  --> [B, 4, dim]\n",
    "        c_n = torch.mul(wenc_n.unsqueeze(1), p.unsqueeze(3)).sum(dim=2)\n",
    "\n",
    "        # [bS, 5-1, dim] -> [bS, 5-1, 3]\n",
    "\n",
    "        vec = torch.cat([self.W_c(c_n), self.W_hs(wenc_hs_ob)], dim=2)\n",
    "        s_wo = self.wo_out(vec)\n",
    "\n",
    "        return s_wo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "Lg5smaEkOHXy"
   },
   "outputs": [],
   "source": [
    "class WVP_se(nn.Module):\n",
    "    \"\"\"\n",
    "    Discriminative model\n",
    "    Get start and end.\n",
    "    Here, classifier for [ [], [1], [2], [], ...]\n",
    "    Input:      Encoded nlu & selected column.\n",
    "    Algorithm: Encoded nlu & selected column. -> classifier -> mask scores -> ...\n",
    "    \"\"\"\n",
    "    def __init__(self, iS=300, hS=100, lS=2, dr=0.3, n_cond_ops=4, old=False):\n",
    "        super(WVP_se, self).__init__()\n",
    "        self.iS = iS\n",
    "        self.hS = hS\n",
    "        self.lS = lS\n",
    "        self.dr = dr\n",
    "        self.n_cond_ops = n_cond_ops\n",
    "        self.question_knowledge_dim = 5\n",
    "        self.header_knowledge_dim = 3\n",
    "        self.mL_w = 4  # max where condition number\n",
    "\n",
    "        self.enc_h = nn.LSTM(input_size=iS, hidden_size=int(hS / 2),\n",
    "                             num_layers=lS, batch_first=True,\n",
    "                             dropout=dr, bidirectional=True)\n",
    "\n",
    "        self.enc_n = nn.LSTM(input_size=iS, hidden_size=int(hS / 2),\n",
    "                             num_layers=lS, batch_first=True,\n",
    "                             dropout=dr, bidirectional=True)\n",
    "\n",
    "        self.W_att = nn.Linear(hS + self.question_knowledge_dim, hS + self.header_knowledge_dim)\n",
    "        self.W_c = nn.Linear(hS + self.question_knowledge_dim, hS)\n",
    "        self.W_hs = nn.Linear(hS + self.header_knowledge_dim, hS)\n",
    "        self.W_op = nn.Linear(n_cond_ops, hS)\n",
    "\n",
    "        # self.W_n = nn.Linear(hS, hS)\n",
    "        if old:\n",
    "            self.wv_out =  nn.Sequential(\n",
    "            nn.Linear(4 * hS, 2)\n",
    "            )\n",
    "        else:\n",
    "            self.wv_out = nn.Sequential(\n",
    "                nn.Linear(4 * hS + self.question_knowledge_dim, hS),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(hS, 2)\n",
    "            )\n",
    "        # self.wv_out = nn.Sequential(\n",
    "        #     nn.Linear(3 * hS, hS),\n",
    "        #     nn.Tanh(),\n",
    "        #     nn.Linear(hS, self.gdkL)\n",
    "        # )\n",
    "\n",
    "        self.softmax_dim1 = nn.Softmax(dim=1)\n",
    "        self.softmax_dim2 = nn.Softmax(dim=2)\n",
    "\n",
    "    def forward(self, wemb_n, l_n, wemb_hpu, l_hpu, l_hs, wn, wc, wo, wenc_n=None, show_p_wv=False,\n",
    "                knowledge=None,\n",
    "                knowledge_header=None):\n",
    "        mL_n = max(l_n)\n",
    "        bS = len(l_hs)\n",
    "        # Encode\n",
    "        if not wenc_n:\n",
    "            wenc_n, hout, cout = encode(self.enc_n, wemb_n, l_n,\n",
    "                            return_hidden=True,\n",
    "                            hc0=None,\n",
    "                            last_only=False)  # [b, n, dim]\n",
    "\n",
    "            knowledge = [k+(mL_n-len(k))*[0] for k in knowledge]\n",
    "            knowledge = torch.tensor(knowledge).unsqueeze(-1)\n",
    "\n",
    "            feature = torch.zeros(bS, mL_n, self.question_knowledge_dim).scatter_(dim=-1,\n",
    "                                                                                  index=knowledge,\n",
    "                                                                                  value=1).to(device)\n",
    "            wenc_n = torch.cat([wenc_n,feature],-1)\n",
    "\n",
    "        wenc_hs = encode_hpu(self.enc_h, wemb_hpu, l_hpu, l_hs)  # [b, hs, dim]\n",
    "\n",
    "        knowledge_header = [k + (max(l_hs) - len(k)) * [0] for k in knowledge_header]\n",
    "        knowledge_header = torch.tensor(knowledge_header).unsqueeze(-1)\n",
    "        feature2 = torch.zeros(bS, max(l_hs), self.header_knowledge_dim).scatter_(dim=-1,\n",
    "                                                                                        index=knowledge_header,\n",
    "                                                                                        value=1).to(device)\n",
    "        wenc_hs = torch.cat([wenc_hs, feature2], -1)\n",
    "\n",
    "\n",
    "        wenc_hs_ob = []  # observed hs\n",
    "\n",
    "        for b in range(bS):\n",
    "            # [[...], [...]]\n",
    "            # Pad list to maximum number of selections\n",
    "            real = [wenc_hs[b, col] for col in wc[b]]\n",
    "            pad = (self.mL_w - wn[b]) * [wenc_hs[b, 0]]  # this padding could be wrong. Test with zero padding later.\n",
    "            wenc_hs_ob1 = torch.stack(real + pad)  # It is not used in the loss function.\n",
    "            wenc_hs_ob.append(wenc_hs_ob1)\n",
    "\n",
    "        # list to [B, 4, dim] tensor.\n",
    "        wenc_hs_ob = torch.stack(wenc_hs_ob)  # list to tensor.\n",
    "        wenc_hs_ob = wenc_hs_ob.to(device)\n",
    "\n",
    "\n",
    "        # Column attention\n",
    "        # [B, 1, mL_n, dim] * [B, 4, dim, 1]\n",
    "        #  -> [B, 4, mL_n, 1] -> [B, 4, mL_n]\n",
    "        # multiplication bewteen NLq-tokens and selected column\n",
    "        att = torch.matmul(self.W_att(wenc_n).unsqueeze(1),\n",
    "                           wenc_hs_ob.unsqueeze(3)\n",
    "                           ).squeeze(3)\n",
    "        # Penalty for blank part.\n",
    "\n",
    "        for b, l_n1 in enumerate(l_n):\n",
    "            if l_n1 < mL_n:\n",
    "                att[b, :, l_n1:] = -10000000000\n",
    "\n",
    "        p = self.softmax_dim2(att)  # p( n| selected_col )\n",
    "\n",
    "        \n",
    "\n",
    "        # [B, 1, mL_n, dim] * [B, 4, mL_n, 1]\n",
    "        #  --> [B, 4, mL_n, dim]\n",
    "        #  --> [B, 4, dim]\n",
    "        c_n = torch.mul(wenc_n.unsqueeze(1), p.unsqueeze(3)).sum(dim=2)\n",
    "\n",
    "        # Select observed headers only.\n",
    "        # Also generate one_hot vector encoding info of the operator\n",
    "        # [B, 4, dim]\n",
    "        wenc_op = []\n",
    "        # print(\"WO E : \",wo)\n",
    "        for b in range(bS):\n",
    "            # print(wo[b])\n",
    "            # [[...], [...]]\n",
    "            # Pad list to maximum number of selections\n",
    "            wenc_op1 = torch.zeros(self.mL_w, self.n_cond_ops)\n",
    "            wo1 = wo[b]\n",
    "            idx_scatter = []\n",
    "            l_wo1 = len(wo1)\n",
    "            for i_wo11 in range(self.mL_w):\n",
    "                if i_wo11 < l_wo1:\n",
    "                    wo11 = wo1[i_wo11]\n",
    "                    idx_scatter.append([int(wo11)])\n",
    "                else:\n",
    "                    idx_scatter.append([0]) # not used anyway\n",
    "\n",
    "            wenc_op1 = wenc_op1.scatter(1, torch.tensor(idx_scatter), 1)\n",
    "\n",
    "            wenc_op.append(wenc_op1)\n",
    "\n",
    "        # list to [B, 4, dim] tensor.\n",
    "        wenc_op = torch.stack(wenc_op)  # list to tensor.\n",
    "        wenc_op = wenc_op.to(device)\n",
    "\n",
    "        # Now after concat, calculate logits for each token\n",
    "        # [bS, 5-1, 3*hS] = [bS, 4, 300]\n",
    "        vec = torch.cat([self.W_c(c_n), self.W_hs(wenc_hs_ob), self.W_op(wenc_op)], dim=2)\n",
    "\n",
    "        # Make extended vector based on encoded nl token containing column and operator information.\n",
    "        # wenc_n = [bS, mL, 100]\n",
    "        # vec2 = [bS, 4, mL, 400]\n",
    "        vec1e = vec.unsqueeze(2).expand(-1,-1, mL_n, -1) # [bS, 4, 1, 300]  -> [bS, 4, mL, 300]\n",
    "        wenc_ne = wenc_n.unsqueeze(1).expand(-1, 4, -1, -1) # [bS, 1, mL, 100] -> [bS, 4, mL, 100]\n",
    "        vec2 = torch.cat( [vec1e, wenc_ne], dim=3)\n",
    "\n",
    "        # now make logits\n",
    "        s_wv = self.wv_out(vec2) # [bS, 4, mL, 400] -> [bS, 4, mL, 2]\n",
    "\n",
    "        # penalty for spurious tokens\n",
    "        for b, l_n1 in enumerate(l_n):\n",
    "            if l_n1 < mL_n:\n",
    "                s_wv[b, :, l_n1:, :] = -10000000000\n",
    "        return s_wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "1d7sUPNvFoJL"
   },
   "outputs": [],
   "source": [
    "# def pred_wo(wn, s_wo):\n",
    "#     \"\"\"\n",
    "#     return: [ pr_wc1_i, pr_wc2_i, ...]\n",
    "#     \"\"\"\n",
    "#     # s_wo = [B, 4, n_op]\n",
    "#     pr_wo_a = s_wo.argmax(dim=2)  # [B, 4]\n",
    "#     # get g_num\n",
    "#     pr_wo = []\n",
    "#     for b, pr_wo_a1 in enumerate(pr_wo_a):\n",
    "#         wn1 = wn[b]\n",
    "#         pr_wo.append(list(pr_wo_a1.data.cpu().numpy()[:wn1]))\n",
    "\n",
    "#     return pr_wo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyVp-XwLSVfx"
   },
   "source": [
    "# LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "IGPxIVJTSVJ4"
   },
   "outputs": [],
   "source": [
    "class Seq2SQL_v1(nn.Module):\n",
    "    def __init__(self, iS, hS, lS, dr, n_cond_ops, n_agg_ops, old=False):\n",
    "        super(Seq2SQL_v1, self).__init__()\n",
    "        self.iS = iS\n",
    "        self.hS = hS\n",
    "        self.ls = lS\n",
    "        self.dr = dr\n",
    "\n",
    "        # print(\"IS = \",iS)\n",
    "\n",
    "        self.max_wn = 4\n",
    "        self.n_cond_ops = n_cond_ops\n",
    "        self.n_agg_ops = n_agg_ops\n",
    "\n",
    "        self.scp = SCP(iS, hS, lS, dr)\n",
    "        self.sap = SAP(iS, hS, lS, dr, n_agg_ops, old=old)\n",
    "        self.wnp = WNP(iS, hS, lS, dr)\n",
    "        self.wcp = WCP(iS, hS, lS, dr)\n",
    "        self.wop = WOP(iS, hS, lS, dr, n_cond_ops)\n",
    "        self.wvp = WVP_se(iS, hS, lS, dr, n_cond_ops, old=old) # start-end-search-discriminative model\n",
    "\n",
    "\n",
    "    def forward(self, wemb_n, l_n, wemb_h, l_hpu, l_hs,\n",
    "                g_sc=None, g_sa=None, g_wn=None, g_wc=None, g_wo=None, g_wvi=None,\n",
    "                show_p_sc=False, show_p_sa=False,\n",
    "                show_p_wn=False, show_p_wc=False, show_p_wo=False, show_p_wv=False,\n",
    "                knowledge = None,\n",
    "                knowledge_header = None):\n",
    "\n",
    "        # sc\n",
    "        s_sc = self.scp(wemb_n, l_n, wemb_h, l_hpu, l_hs, show_p_sc=show_p_sc,\n",
    "                        knowledge=knowledge, knowledge_header=knowledge_header)\n",
    "\n",
    "        \n",
    "        pr_sc = g_sc\n",
    "        \n",
    "        # sa\n",
    "        s_sa = self.sap(wemb_n, l_n, wemb_h, l_hpu, l_hs, pr_sc, show_p_sa=show_p_sa,\n",
    "                        knowledge=knowledge, knowledge_header=knowledge_header)\n",
    "        \n",
    "        pr_sa = g_sa\n",
    "        \n",
    "\n",
    "        # wn\n",
    "        s_wn = self.wnp(wemb_n, l_n, wemb_h, l_hpu, l_hs, show_p_wn=show_p_wn,\n",
    "                        knowledge=knowledge, knowledge_header=knowledge_header)\n",
    "\n",
    "        \n",
    "        pr_wn = g_wn\n",
    "        \n",
    "\n",
    "        # wc\n",
    "        s_wc = self.wcp(wemb_n, l_n, wemb_h, l_hpu, l_hs, show_p_wc=show_p_wc, penalty=True, predict_select_column = pr_sc,\n",
    "                        knowledge=knowledge, knowledge_header=knowledge_header)\n",
    "\n",
    "     \n",
    "        pr_wc = g_wc\n",
    "        # print(\"GWc WVP be = \",g_wc)\n",
    "\n",
    "        # for b, columns in enumerate(pr_wc):\n",
    "        #     for c in columns:\n",
    "        #         s_sc[b, c] = -1e+10\n",
    "\n",
    "        # wo\n",
    "        s_wo = self.wop(wemb_n, l_n, wemb_h, l_hpu, l_hs, wn=pr_wn, wc=pr_wc, show_p_wo=show_p_wo,\n",
    "                        knowledge=knowledge, knowledge_header=knowledge_header)\n",
    "\n",
    "\n",
    "        pr_wo = g_wo\n",
    "\n",
    "        # if g_wo:\n",
    "        #     pr_wo = g_wo\n",
    "        # else:\n",
    "        #     pr_wo = pred_wo(pr_wn, s_wo)\n",
    "\n",
    "        # print(\"GW0 WVP be = \",g_wo)\n",
    "\n",
    "\n",
    "        # wv\n",
    "        s_wv = self.wvp(wemb_n, l_n, wemb_h, l_hpu, l_hs, wn=pr_wn, wc=pr_wc, wo=pr_wo, show_p_wv=show_p_wv,\n",
    "                        knowledge=knowledge, knowledge_header=knowledge_header)\n",
    "\n",
    "        return s_sc, s_sa, s_wn, s_wc, s_wo, s_wv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2lUI_OMxUrMy",
    "outputId": "faa08e1b-4015-47cf-8dd7-c25913a58555"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(hidden_size)\n",
    "print(num_target_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "s-pqe1qZTmKc"
   },
   "outputs": [],
   "source": [
    "model = Seq2SQL_v1(hidden_size * num_target_layers, 100, 2, 0.3, len(cond_ops), len(agg_ops))\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kATEU_7BDnn-"
   },
   "source": [
    "# Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "Z_jawg1LD_UK"
   },
   "outputs": [],
   "source": [
    "def Loss_wc(s_wc, g_wc):\n",
    "\n",
    "    # Construct index matrix\n",
    "    bS, max_h_len = s_wc.shape\n",
    "    im = torch.zeros([bS, max_h_len]).to(device)\n",
    "    for b, g_wc1 in enumerate(g_wc):\n",
    "        for g_wc11 in g_wc1:\n",
    "            im[b, g_wc11] = 1.0\n",
    "    # Construct prob.\n",
    "    p = F.sigmoid(s_wc)\n",
    "    loss = F.binary_cross_entropy(p, im)\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def Loss_wo(s_wo, g_wn, g_wo):\n",
    "\n",
    "    # Construct index matrix\n",
    "    loss = 0\n",
    "    for b, g_wn1 in enumerate(g_wn):\n",
    "        if g_wn1 == 0:\n",
    "            continue\n",
    "        g_wo1 = g_wo[b]\n",
    "        s_wo1 = s_wo[b]\n",
    "        loss += F.cross_entropy(s_wo1[:g_wn1], torch.tensor(g_wo1).to(device))\n",
    "\n",
    "    return loss\n",
    "\n",
    "def Loss_wv_se(s_wv, g_wn, g_wvi):\n",
    "    \"\"\"\n",
    "    s_wv:   [bS, 4, mL, 2], 4 stands for maximum # of condition, 2 tands for start & end logits.\n",
    "    g_wvi:  [ [1, 3, 2], [4,3] ] (when B=2, wn(b=1) = 3, wn(b=2) = 2).\n",
    "    \"\"\"\n",
    "    loss = 0\n",
    "    # g_wvi = torch.tensor(g_wvi).to(device)\n",
    "    for b, g_wvi1 in enumerate(g_wvi):\n",
    "        # for i_wn, g_wvi11 in enumerate(g_wvi1):\n",
    "\n",
    "        g_wn1 = g_wn[b]\n",
    "        if g_wn1 == 0:\n",
    "            continue\n",
    "        g_wvi1 = torch.tensor(g_wvi1).to(device)\n",
    "        g_st1 = g_wvi1[:,0]\n",
    "        g_ed1 = g_wvi1[:,1]\n",
    "        # loss from the start position\n",
    "        loss += F.cross_entropy(s_wv[b,:g_wn1,:,0], g_st1)\n",
    "\n",
    "        # print(\"st_login: \", s_wv[b,:g_wn1,:,0], g_st1, loss)\n",
    "        # loss from the end position\n",
    "        loss += F.cross_entropy(s_wv[b,:g_wn1,:,1], g_ed1)\n",
    "        # print(\"ed_login: \", s_wv[b,:g_wn1,:,1], g_ed1, loss)\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "gOp1HujcDrlp"
   },
   "outputs": [],
   "source": [
    "def Loss_sw_se(s_sc, s_sa, s_wn, s_wc, s_wo, s_wv, g_sc, g_sa, g_wn, g_wc, g_wo, g_wvi):\n",
    "    \"\"\"\n",
    "    :param s_wv: score  [ B, n_conds, T, score]\n",
    "    :param g_wn: [ B ]\n",
    "    :param g_wvi: [B, conds, pnt], e.g. [[[0, 6, 7, 8, 15], [0, 1, 2, 3, 4, 15]], [[0, 1, 2, 3, 16], [0, 7, 8, 9, 16]]]\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    loss = 0\n",
    "    loss += F.cross_entropy(s_sc, torch.tensor(g_sc).to(device))\n",
    "    loss += F.cross_entropy(s_sa, torch.tensor(g_sa).to(device))\n",
    "    loss += F.cross_entropy(s_wn, torch.tensor(g_wn).to(device))\n",
    "    loss += Loss_wc(s_wc, g_wc)\n",
    "    loss += Loss_wo(s_wo, g_wn, g_wo)\n",
    "    loss += Loss_wv_se(s_wv, g_wn, g_wvi)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GMJkGl5rE_dK"
   },
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "VQgnbMGaE-hm"
   },
   "outputs": [],
   "source": [
    "def pred_sw_se(s_sc, s_sa, s_wn, s_wc, s_wo, s_wv):\n",
    "    pr_sc = []\n",
    "    for s_sc1 in s_sc:\n",
    "        pr_sc.append(s_sc1.argmax().item())\n",
    "\n",
    "    pr_sa = []\n",
    "    for s_sa1 in s_sa:\n",
    "        pr_sa.append(s_sa1.argmax().item())\n",
    "\n",
    "    pr_wn = []\n",
    "    for s_wn1 in s_wn:\n",
    "        pr_wn.append(s_wn1.argmax().item())\n",
    "\n",
    "    pr_wc = []\n",
    "    for b, wn1 in enumerate(pr_wn):\n",
    "        s_wc1 = s_wc[b]\n",
    "\n",
    "        pr_wc1 = np.argsort(-s_wc1.data.cpu().numpy())[:wn1]\n",
    "        pr_wc1.sort()\n",
    "\n",
    "        pr_wc.append(list(pr_wc1))\n",
    "\n",
    "    pr_wo_a = s_wo.argmax(dim=2)  # [B, 4]\n",
    "    # get g_num\n",
    "    pr_wo = []\n",
    "    for b, pr_wo_a1 in enumerate(pr_wo_a):\n",
    "        wn1 = pr_wn[b]\n",
    "        pr_wo.append(list(pr_wo_a1.data.cpu().numpy()[:wn1])) \n",
    "\n",
    "\n",
    "\n",
    "    s_wv_st, s_wv_ed = s_wv.split(1, dim=3)  # [B, 4, mL, 2] -> [B, 4, mL, 1], [B, 4, mL, 1]\n",
    "\n",
    "    s_wv_st = s_wv_st.squeeze(3) # [B, 4, mL, 1] -> [B, 4, mL]\n",
    "    s_wv_ed = s_wv_ed.squeeze(3)\n",
    "\n",
    "    pr_wvi_st_idx = s_wv_st.argmax(dim=2) # [B, 4, mL] -> [B, 4, 1]\n",
    "    pr_wvi_ed_idx = s_wv_ed.argmax(dim=2)\n",
    "\n",
    "    pr_wvi = []\n",
    "    for b, wn1 in enumerate(pr_wn):\n",
    "        pr_wvi1 = []\n",
    "        for i_wn in range(wn1):\n",
    "            pr_wvi_st_idx11 = pr_wvi_st_idx[b][i_wn]\n",
    "            pr_wvi_ed_idx11 = pr_wvi_ed_idx[b][i_wn]\n",
    "            pr_wvi1.append([pr_wvi_st_idx11.item(), pr_wvi_ed_idx11.item()])\n",
    "        pr_wvi.append(pr_wvi1)     \n",
    "\n",
    "    return pr_sc, pr_sa, pr_wn, pr_wc, pr_wo, pr_wvi    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "UwCAt4DKG3ps"
   },
   "outputs": [],
   "source": [
    "def convert_pr_wvi_to_string(pr_wvi, nlu_t, nlu_wp_t, wp_to_wh_index, nlu):\n",
    "    \"\"\"\n",
    "    - Convert to the string in whilte-space-separated tokens\n",
    "    - Add-hoc addition.\n",
    "    \"\"\"\n",
    "    pr_wv_str_wp = [] # word-piece version\n",
    "    pr_wv_str = []\n",
    "    for b, pr_wvi1 in enumerate(pr_wvi):\n",
    "        pr_wv_str_wp1 = []\n",
    "        pr_wv_str1 = []\n",
    "        wp_to_wh_index1 = wp_to_wh_index[b]\n",
    "        nlu_wp_t1 = nlu_wp_t[b]\n",
    "        nlu_t1 = nlu_t[b]\n",
    "\n",
    "        for i_wn, pr_wvi11 in enumerate(pr_wvi1):\n",
    "            st_idx, ed_idx = pr_wvi11\n",
    "\n",
    "            # Ad-hoc modification of ed_idx to deal with wp-tokenization effect.\n",
    "            # e.g.) to convert \"butler cc (\" ->\"butler cc (ks)\" (dev set 1st question).\n",
    "            pr_wv_str_wp11 = nlu_wp_t1[st_idx:ed_idx+1]\n",
    "            pr_wv_str_wp1.append(pr_wv_str_wp11)\n",
    "\n",
    "            st_wh_idx = wp_to_wh_index1[st_idx]\n",
    "            ed_wh_idx = wp_to_wh_index1[ed_idx]\n",
    "            pr_wv_str11 = nlu_t1[st_wh_idx:ed_wh_idx+1]\n",
    "\n",
    "            pr_wv_str1.append(pr_wv_str11)\n",
    "\n",
    "        pr_wv_str_wp.append(pr_wv_str_wp1)\n",
    "        pr_wv_str.append(pr_wv_str1)\n",
    "\n",
    "    return pr_wv_str, pr_wv_str_wp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "w7LtvFLvKJkA"
   },
   "outputs": [],
   "source": [
    "def get_cnt_wvi_list(g_wn, g_wc, g_wvi, pr_wvi, mode):\n",
    "    \"\"\" usalbe only when g_wc was used to find pr_wv\n",
    "    \"\"\"\n",
    "    cnt_list =[]\n",
    "    for b, g_wvi1 in enumerate(g_wvi):\n",
    "        g_wc1 = g_wc[b]\n",
    "        pr_wvi1 = pr_wvi[b]\n",
    "        pr_wn1 = len(pr_wvi1)\n",
    "        g_wn1 = g_wn[b]\n",
    "\n",
    "        # Now sorting.\n",
    "        # Sort based wc sequence.\n",
    "        if mode == 'test':\n",
    "            idx1 = np.argsort(np.array(g_wc1))\n",
    "        elif mode == 'train':\n",
    "            idx1 = list( range( g_wn1) )\n",
    "        else:\n",
    "            raise ValueError\n",
    "\n",
    "        if g_wn1 != pr_wn1:\n",
    "            cnt_list.append(0)\n",
    "            continue\n",
    "        else:\n",
    "            flag = True\n",
    "            for i_wn, idx11 in enumerate(idx1):\n",
    "                g_wvi11 = g_wvi1[idx11]\n",
    "                pr_wvi11 = pr_wvi1[i_wn]\n",
    "                if g_wvi11 != pr_wvi11:\n",
    "                    flag = False\n",
    "                    # print(g_wv1, g_wv11)\n",
    "                    # print(pr_wv1, pr_wv11)\n",
    "                    # input('')\n",
    "                    break\n",
    "            if flag:\n",
    "                cnt_list.append(1)\n",
    "            else:\n",
    "                cnt_list.append(0)\n",
    "\n",
    "    return cnt_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "_DOiePTUYLM-"
   },
   "outputs": [],
   "source": [
    "def get_cnt_wv_list(g_wn, g_wc, g_sql_i, pr_sql_i, mode):\n",
    "    \"\"\" usalbe only when g_wc was used to find pr_wv\n",
    "    \"\"\"\n",
    "    cnt_list =[]\n",
    "    for b, g_wc1 in enumerate(g_wc):\n",
    "        pr_wn1 = len(pr_sql_i[b][\"conds\"])\n",
    "        g_wn1 = g_wn[b]\n",
    "\n",
    "        # Now sorting.\n",
    "        # Sort based wc sequence.\n",
    "        if mode == 'test':\n",
    "            idx1 = argsort(array(g_wc1))\n",
    "        elif mode == 'train':\n",
    "            idx1 = list( range( g_wn1) )\n",
    "        else:\n",
    "            raise ValueError\n",
    "\n",
    "        if g_wn1 != pr_wn1:\n",
    "            cnt_list.append(0)\n",
    "            continue\n",
    "        else:\n",
    "            flag = True\n",
    "            for i_wn, idx11 in enumerate(idx1):\n",
    "                g_wvi_str11 = str(g_sql_i[b][\"conds\"][idx11][2]).lower()\n",
    "                pr_wvi_str11 = str(pr_sql_i[b][\"conds\"][i_wn][2]).lower()\n",
    "                # print(g_wvi_str11)\n",
    "                # print(pr_wvi_str11)\n",
    "                # print(g_wvi_str11==pr_wvi_str11)\n",
    "                if g_wvi_str11 != pr_wvi_str11:\n",
    "                    flag = False\n",
    "                    # print(g_wv1, g_wv11)\n",
    "                    # print(pr_wv1, pr_wv11)\n",
    "                    # input('')\n",
    "                    break\n",
    "            if flag:\n",
    "                cnt_list.append(1)\n",
    "            else:\n",
    "                cnt_list.append(0)\n",
    "\n",
    "    return cnt_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "xjG3f6iDHWra"
   },
   "outputs": [],
   "source": [
    "def get_cnt_sw_list(g_sc, g_sa, g_wn, g_wc, g_wo, g_wvi,\n",
    "                    pr_sc, pr_sa, pr_wn, pr_wc, pr_wo, pr_wvi,\n",
    "                    g_sql_i, pr_sql_i,\n",
    "                    mode):\n",
    "    \"\"\" usalbe only when g_wc was used to find pr_wv\n",
    "    \"\"\"\n",
    "    \n",
    "    cnt_sc = []\n",
    "    for b1, g_sc1 in enumerate(g_sc):\n",
    "        pr_sc1 = pr_sc[b1]\n",
    "        if pr_sc1 == g_sc1:\n",
    "            cnt_sc.append(1)\n",
    "        else:\n",
    "            cnt_sc.append(0)\n",
    "\n",
    "    # cnt_sa = get_cnt_sc_list(g_sa, pr_sa)\n",
    "    cnt_sa=[]\n",
    "    for b2, g_sa1 in enumerate(g_sa):\n",
    "        pr_sa1 = pr_sa[b2]\n",
    "        if pr_sa1 == g_sa1:\n",
    "            cnt_sa.append(1)\n",
    "        else:\n",
    "            cnt_sa.append(0)\n",
    "\n",
    "    # cnt_wn = get_cnt_sc_list(g_wn, pr_wn)\n",
    "    cnt_wn=[]\n",
    "    for b3, g_wn1 in enumerate(g_wn):\n",
    "        pr_wn1 = pr_wn[b3]\n",
    "        if pr_wn1 == g_wn1:\n",
    "            cnt_wn.append(1)\n",
    "        else:\n",
    "            cnt_wn.append(0)\n",
    "\n",
    "    # cnt_wc = get_cnt_wc_list(g_wc, pr_wc)\n",
    "\n",
    "    cnt_wc= []\n",
    "    for b4, g_wc1 in enumerate(g_wc):\n",
    "\n",
    "        pr_wc1 = pr_wc[b4]\n",
    "        pr_wn1 = len(pr_wc1)\n",
    "        g_wn1 = len(g_wc1)\n",
    "\n",
    "        if pr_wn1 != g_wn1:\n",
    "            cnt_wc.append(0)\n",
    "            continue\n",
    "        else:\n",
    "            wc1 = np.array(g_wc1)\n",
    "            wc1.sort()\n",
    "\n",
    "            if np.array_equal(pr_wc1, wc1):\n",
    "                cnt_wc.append(1)\n",
    "            else:\n",
    "                cnt_wc.append(0)\n",
    "\n",
    "    # cnt_wo = get_cnt_wo_list(g_wn, g_wc, g_wo, pr_wc, pr_wo, mode)\n",
    "\n",
    "    cnt_wo=[]\n",
    "    for b, g_wo1 in enumerate(g_wo):\n",
    "        g_wc1 = g_wc[b]\n",
    "        pr_wc1 = pr_wc[b]\n",
    "        pr_wo1 = pr_wo[b]\n",
    "        pr_wn1 = len(pr_wo1)\n",
    "        g_wn1 = g_wn[b]\n",
    "\n",
    "        if g_wn1 != pr_wn1:\n",
    "            cnt_wo.append(0)\n",
    "            continue\n",
    "        else:\n",
    "            # Sort based wc sequence.\n",
    "            if mode == 'test':\n",
    "                idx = np.argsort(np.array(g_wc1))\n",
    "\n",
    "                g_wo1_s = np.array(g_wo1)[idx]\n",
    "                g_wo1_s = list(g_wo1_s)\n",
    "            elif mode == 'train':\n",
    "                # due to tearch forcing, no need to sort.\n",
    "                g_wo1_s = g_wo1\n",
    "            else:\n",
    "                raise ValueError\n",
    "\n",
    "            if type(pr_wo1) != list:\n",
    "                raise TypeError\n",
    "            if g_wo1_s == pr_wo1:\n",
    "                cnt_wo.append(1)\n",
    "            else:\n",
    "                cnt_wo.append(0)\n",
    "    \n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    if pr_wvi:\n",
    "        cnt_wvi = get_cnt_wvi_list(g_wn, g_wc, g_wvi, pr_wvi, mode)\n",
    "    else:\n",
    "        cnt_wvi = [0]*len(cnt_sc)\n",
    "    cnt_wv = get_cnt_wv_list(g_wn, g_wc, g_sql_i, pr_sql_i, mode) # compare using wv-str which presented in original data.\n",
    "\n",
    "\n",
    "    return cnt_sc, cnt_sa, cnt_wn, cnt_wc, cnt_wo, cnt_wvi, cnt_wv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "w-RDGznnSQ1x"
   },
   "outputs": [],
   "source": [
    "def sort_pr_wc(pr_wc, g_wc):\n",
    "    \"\"\"\n",
    "    Input: list\n",
    "    pr_wc = [B, n_conds]\n",
    "    g_wc = [B, n_conds]\n",
    "    Return: list\n",
    "    pr_wc_sorted = [B, n_conds]\n",
    "    \"\"\"\n",
    "    pr_wc_sorted = []\n",
    "    for b, pr_wc1 in enumerate(pr_wc):\n",
    "        g_wc1 = g_wc[b]\n",
    "        pr_wc1_sorted = []\n",
    "\n",
    "        if set(g_wc1) == set(pr_wc1):\n",
    "            pr_wc1_sorted = copy.deepcopy(g_wc1)\n",
    "        else:\n",
    "            # no sorting when g_wc1 and pr_wc1 are different.\n",
    "            pr_wc1_sorted = copy.deepcopy(pr_wc1)\n",
    "\n",
    "        pr_wc_sorted.append(pr_wc1_sorted)\n",
    "    return pr_wc_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "oaRNKD4HSqPy"
   },
   "outputs": [],
   "source": [
    "def generate_sql_i(pr_sc, pr_sa, pr_wn, pr_wc, pr_wo, pr_wv_str, nlu):\n",
    "    pr_sql_i = []\n",
    "    for b, nlu1 in enumerate(nlu):\n",
    "        conds = []\n",
    "        for i_wn in range(pr_wn[b]):\n",
    "            conds1 = []\n",
    "            conds1.append(pr_wc[b][i_wn])\n",
    "            conds1.append(pr_wo[b][i_wn])\n",
    "            merged_wv11 = merge_wv_t1_eng(pr_wv_str[b][i_wn], nlu[b])\n",
    "            conds1.append(merged_wv11)\n",
    "            conds.append(conds1)\n",
    "\n",
    "        pr_sql_i1 = {'agg': pr_sa[b], 'sel': pr_sc[b], 'conds': conds}\n",
    "        pr_sql_i.append(pr_sql_i1)\n",
    "    return pr_sql_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "6fk6oZ0iUKd6"
   },
   "outputs": [],
   "source": [
    "def merge_wv_t1_eng(where_str_tokens, NLq):\n",
    "    \"\"\"\n",
    "    Almost copied of SQLNet.\n",
    "    The main purpose is pad blank line while combining tokens.\n",
    "    \"\"\"\n",
    "    nlq = NLq.lower()\n",
    "    where_str_tokens = [tok.lower() for tok in where_str_tokens]\n",
    "    alphabet = 'abcdefghijklmnopqrstuvwxyz0123456789$'\n",
    "    special = {'-LRB-': '(',\n",
    "               '-RRB-': ')',\n",
    "               '-LSB-': '[',\n",
    "               '-RSB-': ']',\n",
    "               '``': '\"',\n",
    "               '\\'\\'': '\"',\n",
    "               }\n",
    "               # '--': '\\u2013'} # this generate error for test 5661 case.\n",
    "    ret = ''\n",
    "    double_quote_appear = 0\n",
    "    for raw_w_token in where_str_tokens:\n",
    "        # if '' (empty string) of None, continue\n",
    "        if not raw_w_token:\n",
    "            continue\n",
    "\n",
    "        # Change the special characters\n",
    "        w_token = special.get(raw_w_token, raw_w_token)  # maybe necessary for some case?\n",
    "\n",
    "        # check the double quote\n",
    "        if w_token == '\"':\n",
    "            double_quote_appear = 1 - double_quote_appear\n",
    "\n",
    "        # Check whether ret is empty. ret is selected where condition.\n",
    "        if len(ret) == 0:\n",
    "            pass\n",
    "        # Check blank character.\n",
    "        elif len(ret) > 0 and ret + ' ' + w_token in nlq:\n",
    "            # Pad ' ' if ret + ' ' is part of nlq.\n",
    "            ret = ret + ' '\n",
    "\n",
    "        elif len(ret) > 0 and ret + w_token in nlq:\n",
    "            pass  # already in good form. Later, ret + w_token will performed.\n",
    "\n",
    "        # Below for unnatural question I guess. Is it likely to appear?\n",
    "        elif w_token == '\"':\n",
    "            if double_quote_appear:\n",
    "                ret = ret + ' '  # pad blank line between next token when \" because in this case, it is of closing apperas\n",
    "                # for the case of opening, no blank line.\n",
    "\n",
    "        elif w_token[0] not in alphabet:\n",
    "            pass  # non alphabet one does not pad blank line.\n",
    "\n",
    "        # when previous character is the special case.\n",
    "        elif (ret[-1] not in ['(', '/', '\\u2013', '#', '$', '&']) and (ret[-1] != '\"' or not double_quote_appear):\n",
    "            ret = ret + ' '\n",
    "        ret = ret + w_token\n",
    "\n",
    "    return ret.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BOsqLHNRbbzS"
   },
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "PHGs_ac07vva"
   },
   "outputs": [],
   "source": [
    "def train(train_loader, train_table, model, model_bert, hidden_size, num_hidden_layers,\n",
    "          max_seq_length): #opt,model\n",
    "    model.train()\n",
    "    model_bert.train()\n",
    "\n",
    "    ave_loss = 0\n",
    "    cnt = 0  # count the # of examples\n",
    "    cnt_sc = 0  # count the # of correct predictions of select column\n",
    "    cnt_sa = 0  # of selectd aggregation\n",
    "    cnt_wn = 0  # of where number\n",
    "    cnt_wc = 0  # of where column\n",
    "    cnt_wo = 0  # of where operator\n",
    "    cnt_wv = 0  # of where-value\n",
    "    cnt_wvi = 0  # of where-value index (on question tokens)\n",
    "    cnt_lx = 0  # of logical form acc\n",
    "    cnt_x = 0  # of execution acc\n",
    "\n",
    "\n",
    "    for iB, t in enumerate(train_loader):\n",
    "        if iB % 100 == 0:\n",
    "            print(\"i = \",iB,\"/5636\")\n",
    "        cnt += len(t)\n",
    "\n",
    "        # Get fields\n",
    "        nlu, nlu_t, sql_i, sql_q, tb, hds,g_wvi_corenlp = get_fields(t, train_table)\n",
    "        # print('get nlu_t = ',nlu_t,type(nlu_t))\n",
    "\n",
    "        g_sc, g_sa, g_wn, g_wc, g_wo, g_wv = get_g(sql_i)\n",
    "        # print(\"GWC_FIRST = \",g_wo,len(g_wo),len(g_wo[0]))\n",
    "\n",
    "        \n",
    "\n",
    "        wemb_n, wemb_h, l_n, l_hpu, l_hs,nlu_tt, t_to_tt_idx, tt_to_t_idx = get_wemb_bert(hidden_size, num_hidden_layers, model_bert, nlu_t, hds, max_seq_length)\n",
    "\n",
    "        # print(wemb_n.shape)\n",
    "        # print(wemb_h.shape)\n",
    "\n",
    "        g_wvi_corenlp=[]\n",
    "\n",
    "\n",
    "\n",
    "        knowledge = []\n",
    "        for k in t:\n",
    "            \n",
    "            knowledge.append(max(l_n)*[0])\n",
    "\n",
    "        knowledge_header = []\n",
    "        for k in t:\n",
    "            knowledge_header.append(max(l_hs) * [0])\n",
    "\n",
    "\n",
    "\n",
    "        for t1 in t:\n",
    "            g_wvi_corenlp.append( t1['wvi_corenlp'] ) \n",
    "            # knowledge.append(t1[\"bertindex_knowledge\"])\n",
    "            # knowledge_header.append(t1[\"header_knowledge\"])\n",
    "\n",
    "\n",
    "\n",
    "        try:\n",
    "            g_wvi = get_g_wvi_bert_from_g_wvi_corenlp(t_to_tt_idx, g_wvi_corenlp)\n",
    "        except:\n",
    "          # removing examples in which \"where\" condition is absent\n",
    "            continue\n",
    "\n",
    "        # print(\"\\n ent\\n\")\n",
    "        s_sc, s_sa, s_wn, s_wc, s_wo, s_wv = model(wemb_n, l_n, wemb_h, l_hpu, l_hs,\n",
    "                                                   g_sc=g_sc, g_sa=g_sa, g_wn=g_wn, g_wc=g_wc, g_wvi=g_wvi,g_wo=g_wo,\n",
    "                                                   knowledge = knowledge,\n",
    "                                                   knowledge_header = knowledge_header)  \n",
    "\n",
    "        # g_wo not passed above check later\n",
    "          \n",
    "        loss = Loss_sw_se(s_sc, s_sa, s_wn, s_wc, s_wo, s_wv, g_sc, g_sa, g_wn, g_wc, g_wo, g_wvi)\n",
    "\n",
    "        if iB % accumulate_gradients == 0:  # mode\n",
    "            # at start, perform zero_grad\n",
    "            opt.zero_grad()\n",
    "            if opt_bert:\n",
    "                opt_bert.zero_grad()\n",
    "            loss.backward()\n",
    "            if accumulate_gradients == 1:\n",
    "                opt.step()\n",
    "                if opt_bert:\n",
    "                    opt_bert.step()\n",
    "        elif iB % accumulate_gradients == (accumulate_gradients - 1):\n",
    "            # at the final, take step with accumulated graident\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            if opt_bert:\n",
    "                opt_bert.step()\n",
    "        else:\n",
    "            # at intermediate stage, just accumulates the gradients\n",
    "            loss.backward()\n",
    " \n",
    "\n",
    "        pr_sc, pr_sa, pr_wn, pr_wc, pr_wo, pr_wvi = pred_sw_se(s_sc, s_sa, s_wn, s_wc, s_wo, s_wv, )\n",
    "        pr_wv_str, pr_wv_str_wp = convert_pr_wvi_to_string(pr_wvi, nlu_t, nlu_tt, tt_to_t_idx, nlu)\n",
    "\n",
    "        pr_wc_sorted = sort_pr_wc(pr_wc, g_wc)\n",
    "        pr_sql_i = generate_sql_i(pr_sc, pr_sa, pr_wn, pr_wc_sorted, pr_wo, pr_wv_str, nlu)\n",
    "\n",
    "\n",
    "        cnt_sc1_list, cnt_sa1_list, cnt_wn1_list,cnt_wc1_list, cnt_wo1_list,cnt_wvi1_list, cnt_wv1_list = get_cnt_sw_list(g_sc, g_sa, g_wn, g_wc, g_wo, g_wvi,\n",
    "                                                      pr_sc, pr_sa, pr_wn, pr_wc, pr_wo, pr_wvi,\n",
    "                                                      sql_i, pr_sql_i,\n",
    "                                                      mode='train')\n",
    "        \n",
    "        ave_loss += loss.item()\n",
    "\n",
    "        # count\n",
    "        cnt_sc += sum(cnt_sc1_list)\n",
    "        cnt_sa += sum(cnt_sa1_list)\n",
    "        cnt_wn += sum(cnt_wn1_list)\n",
    "        cnt_wc += sum(cnt_wc1_list)\n",
    "        cnt_wo += sum(cnt_wo1_list)\n",
    "        cnt_wvi += sum(cnt_wvi1_list)\n",
    "        cnt_wv += sum(cnt_wv1_list)\n",
    "\n",
    "    ave_loss /= cnt\n",
    "    acc_sc = cnt_sc / cnt\n",
    "    acc_sa = cnt_sa / cnt\n",
    "    acc_wn = cnt_wn / cnt\n",
    "    acc_wc = cnt_wc / cnt\n",
    "    acc_wo = cnt_wo / cnt\n",
    "    acc_wvi = cnt_wvi / cnt\n",
    "    acc_wv = cnt_wv / cnt\n",
    "\n",
    "    acc = [ave_loss, acc_sc, acc_sa, acc_wn, acc_wc, acc_wo, acc_wvi, acc_wv]\n",
    "\n",
    "    aux_out = 1\n",
    "\n",
    "    return acc, aux_out  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "IPHOsBseo-U_"
   },
   "outputs": [],
   "source": [
    "accumulate_gradients = 1\n",
    "opt = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()),\n",
    "                               lr=1e-3, weight_decay=0)\n",
    "\n",
    "opt_bert = torch.optim.Adam(filter(lambda p: p.requires_grad, model_bert.parameters()),\n",
    "                            lr=1e-5, weight_decay=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U0Nsrj4ZLh_n",
    "outputId": "29071ef2-ef14-4b90-87ff-eb41616c5919"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i =  0 /5636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1639: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i =  100 /5636\n",
      "i =  200 /5636\n",
      "i =  300 /5636\n",
      "i =  400 /5636\n",
      "i =  500 /5636\n",
      "i =  600 /5636\n",
      "i =  700 /5636\n",
      "i =  800 /5636\n",
      "i =  900 /5636\n",
      "i =  1000 /5636\n",
      "i =  1100 /5636\n",
      "i =  1200 /5636\n",
      "i =  1300 /5636\n",
      "i =  1400 /5636\n",
      "i =  1500 /5636\n",
      "i =  1600 /5636\n",
      "i =  1700 /5636\n",
      "i =  1800 /5636\n",
      "i =  1900 /5636\n",
      "i =  2000 /5636\n",
      "i =  2100 /5636\n",
      "i =  2200 /5636\n",
      "i =  2300 /5636\n",
      "i =  2400 /5636\n",
      "i =  2500 /5636\n",
      "i =  2600 /5636\n",
      "i =  2700 /5636\n",
      "i =  2800 /5636\n",
      "i =  2900 /5636\n",
      "i =  3000 /5636\n",
      "i =  3100 /5636\n",
      "i =  3200 /5636\n",
      "i =  3300 /5636\n",
      "i =  3400 /5636\n",
      "i =  3500 /5636\n",
      "i =  3600 /5636\n",
      "i =  3700 /5636\n",
      "i =  3800 /5636\n",
      "i =  3900 /5636\n",
      "i =  4000 /5636\n",
      "i =  4100 /5636\n",
      "i =  4200 /5636\n",
      "i =  4300 /5636\n",
      "i =  4400 /5636\n",
      "i =  4500 /5636\n",
      "i =  4600 /5636\n",
      "i =  4700 /5636\n",
      "i =  4800 /5636\n",
      "i =  4900 /5636\n",
      "i =  5000 /5636\n",
      "i =  5100 /5636\n",
      "i =  5200 /5636\n",
      "i =  5300 /5636\n",
      "i =  5400 /5636\n",
      "i =  5500 /5636\n",
      "i =  5600 /5636\n"
     ]
    }
   ],
   "source": [
    "acc, aux_out  = train(train_loader, train_table, model, model_bert, hidden_size, num_hidden_layers,max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FrFzpswqq0eu",
    "outputId": "0ffae9ff-c8bd-4508-a99e-56682258ed4e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.3706215210153854,\n",
       " 0.24943660722207436,\n",
       " 0.7558158104870908,\n",
       " 0.8567651494987135,\n",
       " 0.18198917576080206,\n",
       " 0.7586904445035932,\n",
       " 0.4261733652737113,\n",
       " 0.491367225623281]"
      ]
     },
     "execution_count": 76,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "ns5xl-mUMFAw"
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), '/content/drive/MyDrive/Text2SQL/model_v1.pt')\n",
    "torch.save(model_bert.state_dict(), '/content/drive/MyDrive/Text2SQL/bert_v1.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YzU-ahD7V5TO"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Text2SQL.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
